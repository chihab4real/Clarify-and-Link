{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "58bbb387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import warnings\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3851bc",
   "metadata": {},
   "source": [
    "Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "67df7a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"hf://datasets/cyanic-selkie/aida-conll-yago-wikidata/\"\n",
    "splits = {'train': 'train.parquet', 'validation': 'validation.parquet', 'test': 'test.parquet'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fcdaf814",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_parquet(base_url + splits['train'])\n",
    "df_val = pd.read_parquet(base_url + splits['validation'])\n",
    "df_test = pd.read_parquet(base_url + splits['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4e987b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 946 examples\n",
      "Validation: 216 examples\n",
      "Test: 231 examples\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train: {len(df_train)} examples\")\n",
    "print(f\"Validation: {len(df_val)} examples\")\n",
    "print(f\"Test: {len(df_test)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c873a4",
   "metadata": {},
   "source": [
    "Showing Original Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7bc7175c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (946, 3)\n",
      "Columns: ['document_id', 'text', 'entities']\n",
      "\n",
      "First row text length: 2790\n",
      "First row entities: 48\n",
      "First entity: {'start': 0, 'end': 2, 'tag': 'ORG', 'pageid': None, 'qid': None, 'title': None}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train shape: {df_train.shape}\")\n",
    "print(f\"Columns: {df_train.columns.tolist()}\")\n",
    "print(f\"\\nFirst row text length: {len(df_train.iloc[0]['text'])}\")\n",
    "print(f\"First row entities: {len(df_train.iloc[0]['entities'])}\")\n",
    "\n",
    "if len(df_train.iloc[0]['entities']) > 0:\n",
    "    print(f\"First entity: {df_train.iloc[0]['entities'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c42710c",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114143ce",
   "metadata": {},
   "source": [
    "Apply preprocessing to a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "49ed095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_train.head(100).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c339a39a",
   "metadata": {},
   "source": [
    "Step 1: Filter valid Entities (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "80adb819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_valid_entities(row):\n",
    "    entities = row['entities'] if 'entities' in row and row['entities'] is not None else []\n",
    "    \n",
    "    valid_entities = []\n",
    "    for entity in entities:\n",
    "        if entity.get('qid') is not None or entity.get('pageid') is not None:\n",
    "            valid_entities.append(entity)\n",
    "    \n",
    "    return valid_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9ad62e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_filter_valid_entities(df, inplace=False):\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "    \n",
    "    print(\"Filtering entities without KB links...\")\n",
    "    df['entities'] = df.apply(filter_valid_entities, axis=1)\n",
    "    df['num_valid_entities'] = df['entities'].apply(len)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7e6f28d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sample = apply_filter_valid_entities(df_sample, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b64e7b",
   "metadata": {},
   "source": [
    "Step 2: Add context to entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "19330973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mention_with_context(text, start, end, context_window=50):\n",
    "    mention = text[start:end]\n",
    "    \n",
    "    # Get left and right context\n",
    "    left_start = max(0, start - context_window)\n",
    "    right_end = min(len(text), end + context_window)\n",
    "    \n",
    "    left_context = text[left_start:start]\n",
    "    right_context = text[end:right_end]\n",
    "    full_context = text[left_start:right_end]\n",
    "    \n",
    "    return {\n",
    "        'mention': mention,\n",
    "        'left_context': left_context,\n",
    "        'right_context': right_context,\n",
    "        'full_context': full_context,\n",
    "        'mention_start': start,\n",
    "        'mention_end': end\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2612ac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_context_to_entities(row, context_window=50):\n",
    "    text = row['text']\n",
    "    entities = row.get('entities', [])\n",
    "    \n",
    "    if entities is None or len(entities) == 0:\n",
    "        return []\n",
    "    \n",
    "    entities_with_context = []\n",
    "    for entity in entities:\n",
    "        entity_copy = entity.copy()\n",
    "        context_info = extract_mention_with_context(\n",
    "            text, \n",
    "            entity['start'], \n",
    "            entity['end'], \n",
    "            context_window\n",
    "        )\n",
    "        entity_copy.update(context_info)\n",
    "        entities_with_context.append(entity_copy)\n",
    "    \n",
    "    return entities_with_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "51120f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_add_context(df, context_window=50, inplace=False):\n",
    "    if inplace is False:\n",
    "        df = df.copy()\n",
    "    \n",
    "    print(f\"Adding context (window={context_window}) to entities...\")\n",
    "    df['entities'] = df.apply(\n",
    "        lambda row: add_context_to_entities(row, context_window), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6845dac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding context (window=200) to entities...\n"
     ]
    }
   ],
   "source": [
    "df_sample = apply_add_context(df_sample, context_window=200, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c3caf0",
   "metadata": {},
   "source": [
    "Step 3: Normalize mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "42cf14f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_mention(mention):\n",
    "    # Remove extra whitespace\n",
    "    normalized = re.sub(r'\\s+', ' ', mention).strip()\n",
    "    \n",
    "    # Remove leading/trailing punctuation\n",
    "    normalized = re.sub(r'^[^\\w]+|[^\\w]+$', '', normalized)\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "342b36e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_normalized_mentions(row):\n",
    "    entities = row.get('entities', [])\n",
    "    \n",
    "    if not entities:\n",
    "        return []\n",
    "    \n",
    "    entities_normalized = []\n",
    "    for entity in entities:\n",
    "        entity_copy = entity.copy()\n",
    "        if 'mention' in entity:\n",
    "            entity_copy['normalized_mention'] = normalize_mention(entity['mention'])\n",
    "        entities_normalized.append(entity_copy)\n",
    "    \n",
    "    return entities_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7525e123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_normalize_mentions(df, inplace=False):\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "    \n",
    "    print(\"Normalizing entity mentions...\")\n",
    "    df['entities'] = df.apply(add_normalized_mentions, axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "910fa102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing entity mentions...\n"
     ]
    }
   ],
   "source": [
    "df_sample = apply_normalize_mentions(df_sample, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9423a9",
   "metadata": {},
   "source": [
    "Step 4: Remove overlapping entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "61b1495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_overlapping_entities(row):\n",
    "    entities = row.get('entities', [])\n",
    "    if not entities:\n",
    "        return [], 0\n",
    "    \n",
    "    # Sort by start position, then by length (descending)\n",
    "    sorted_entities = sorted(\n",
    "        entities, \n",
    "        key=lambda e: (e['start'], -(e['end'] - e['start']))\n",
    "    )\n",
    "    \n",
    "    non_overlapping = []\n",
    "    last_end = -1\n",
    "    \n",
    "    for entity in sorted_entities:\n",
    "        if entity['start'] >= last_end:\n",
    "            non_overlapping.append(entity)\n",
    "            last_end = entity['end']\n",
    "    \n",
    "    num_removed = len(entities) - len(non_overlapping)\n",
    "    \n",
    "    return non_overlapping, num_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ee4cc15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_remove_overlaps(df, inplace=False):\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "    \n",
    "    print(\"Removing overlapping entities...\")\n",
    "    result = df.apply(remove_overlapping_entities, axis=1)\n",
    "    df['entities'] = result.apply(lambda x: x[0])\n",
    "    df['removed_overlaps'] = result.apply(lambda x: x[1])\n",
    "    \n",
    "    total_removed = df['removed_overlaps'].sum()\n",
    "    print(f\"  Removed {total_removed} overlapping entities\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "396294ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing overlapping entities...\n",
      "  Removed 0 overlapping entities\n"
     ]
    }
   ],
   "source": [
    "df_sample = apply_remove_overlaps(df_sample, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fad163f",
   "metadata": {},
   "source": [
    "Step 5: Create candidate pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d10d2d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mention_candidate_pairs(row, max_candidates=10):\n",
    "    \n",
    "    pairs = []\n",
    "    \n",
    "    for entity in row.get('entities', []):\n",
    "        pair = {\n",
    "            'mention': entity.get('mention', ''),\n",
    "            'context': entity.get('full_context', ''),\n",
    "            'entity_type': entity.get('tag', ''),\n",
    "            'true_qid': entity.get('qid'),\n",
    "            'true_pageid': entity.get('pageid'),\n",
    "            'true_title': entity.get('title'),\n",
    "            # we'd generate candidates here\n",
    "            'candidates': []  #for candidate entities\n",
    "        }\n",
    "        pairs.append(pair)\n",
    "    \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "20c6d150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_create_candidate_pairs(df, max_candidates=10, inplace=False):\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "    \n",
    "    print(\"Creating mention-candidate pairs...\")\n",
    "    df['mention_candidate_pairs'] = df.apply(\n",
    "        lambda row: create_mention_candidate_pairs(row, max_candidates),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dcb23a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating mention-candidate pairs...\n"
     ]
    }
   ],
   "source": [
    "df_sample = apply_create_candidate_pairs(df_sample, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0254fc81",
   "metadata": {},
   "source": [
    "Step 6: Create NIL detection examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "36d7682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nil_detection_examples(row):\n",
    "    nil_examples = []\n",
    "    linked_examples = []\n",
    "    \n",
    "    for entity in row.get('entities', []):\n",
    "        entity_example = {\n",
    "            'mention': entity.get('mention', ''),\n",
    "            'context': entity.get('full_context', ''),\n",
    "            'entity_type': entity.get('tag', ''),\n",
    "            'is_nil': entity.get('qid') is None and entity.get('pageid') is None\n",
    "        }\n",
    "        \n",
    "        if entity_example['is_nil']:\n",
    "            nil_examples.append(entity_example)\n",
    "        else:\n",
    "            linked_examples.append(entity_example)\n",
    "    \n",
    "    return nil_examples, linked_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6108989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_create_nil_examples(df, inplace=False):\n",
    "    \"\"\"\n",
    "    Create NIL detection examples for entire DataFrame.\n",
    "    \"\"\"\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "    \n",
    "    print(\"Creating NIL detection examples...\")\n",
    "    result = df.apply(create_nil_detection_examples, axis=1)\n",
    "    df['nil_examples'] = result.apply(lambda x: x[0])\n",
    "    df['linked_examples'] = result.apply(lambda x: x[1])\n",
    "    \n",
    "    total_nil = df['nil_examples'].apply(len).sum()\n",
    "    total_linked = df['linked_examples'].apply(len).sum()\n",
    "    print(f\"  NIL entities: {total_nil}, Linked entities: {total_linked}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "09a29bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating NIL detection examples...\n",
      "  NIL entities: 504, Linked entities: 2139\n"
     ]
    }
   ],
   "source": [
    "df_sample = apply_create_nil_examples(df_sample, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15aa076",
   "metadata": {},
   "source": [
    "tep 7: Split long documents (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "05d14e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_long_documents(row, max_length=512, overlap=50):\n",
    "    text = row['text']\n",
    "    entities = row.get('entities', [])\n",
    "    \n",
    "    if len(text) <= max_length:\n",
    "        return [row.to_dict()]  # No splitting needed\n",
    "    \n",
    "    chunks = []\n",
    "    start_pos = 0\n",
    "    chunk_id = 0\n",
    "    \n",
    "    while start_pos < len(text):\n",
    "        end_pos = min(start_pos + max_length, len(text))\n",
    "        chunk_text = text[start_pos:end_pos]\n",
    "        \n",
    "        # Find entities in this chunk\n",
    "        chunk_entities = []\n",
    "        for entity in entities:\n",
    "            if entity['start'] >= start_pos and entity['end'] <= end_pos:\n",
    "                # Adjust entity positions relative to chunk\n",
    "                adjusted_entity = entity.copy()\n",
    "                adjusted_entity['start'] = entity['start'] - start_pos\n",
    "                adjusted_entity['end'] = entity['end'] - start_pos\n",
    "                chunk_entities.append(adjusted_entity)\n",
    "        \n",
    "        chunk = {\n",
    "            'text': chunk_text,\n",
    "            'entities': chunk_entities,\n",
    "            'chunk_start': start_pos,\n",
    "            'chunk_end': end_pos,\n",
    "            'chunk_id': chunk_id,\n",
    "            'is_chunk': True\n",
    "        }\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "        chunk_id += 1\n",
    "        # Move to next chunk with overlap\n",
    "        start_pos = end_pos - overlap\n",
    "        if start_pos >= len(text) - overlap:\n",
    "            break\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fc7a7c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_split_long_documents(df, max_length=512, overlap=50):\n",
    "    all_chunks = []\n",
    "    for idx, row in df.iterrows():\n",
    "        chunks = split_long_documents(row, max_length, overlap)\n",
    "        for chunk in chunks:\n",
    "            # Preserve original index info\n",
    "            chunk['original_index'] = idx\n",
    "            all_chunks.append(chunk)\n",
    "    \n",
    "    df_chunks = pd.DataFrame(all_chunks)\n",
    "    \n",
    "    print(f\"  Original docs: {len(df)}, After splitting: {len(df_chunks)}\")\n",
    "    \n",
    "    return df_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "34d0136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sample = apply_split_long_documents(df_sample, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a8db5a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed shape: (100, 7)\n",
      "New columns: ['document_id', 'text', 'entities', 'removed_overlaps', 'mention_candidate_pairs', 'nil_examples', 'linked_examples']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Processed shape: {df_sample.shape}\")\n",
    "print(f\"New columns: {df_sample.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49249171",
   "metadata": {},
   "source": [
    "Export the Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "63959d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.to_parquet(\"preprocessed_sample_aida.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9103b9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = df_sample[['mention_candidate_pairs', 'nil_examples', 'linked_examples']].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ec4b995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_mcp = x['mention_candidate_pairs'].explode().apply(pd.Series)\n",
    "# df_mcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef52007d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
