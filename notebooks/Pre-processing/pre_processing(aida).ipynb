{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58bbb387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e479fc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "\n",
    "\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "import Utils as u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3851bc",
   "metadata": {},
   "source": [
    "Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67df7a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"hf://datasets/cyanic-selkie/aida-conll-yago-wikidata/\"\n",
    "splits = {'train': 'train.parquet', 'validation': 'validation.parquet', 'test': 'test.parquet'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcdaf814",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_parquet(base_url + splits['train'])\n",
    "df_val = pd.read_parquet(base_url + splits['validation'])\n",
    "df_test = pd.read_parquet(base_url + splits['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e987b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 946 examples\n",
      "Validation: 216 examples\n",
      "Test: 231 examples\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train: {len(df_train)} examples\")\n",
    "print(f\"Validation: {len(df_val)} examples\")\n",
    "print(f\"Test: {len(df_test)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c873a4",
   "metadata": {},
   "source": [
    "Showing Original Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bc7175c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (946, 3)\n",
      "Columns: ['document_id', 'text', 'entities']\n",
      "\n",
      "First row text length: 2790\n",
      "First row entities: 48\n",
      "First entity: {'start': 0, 'end': 2, 'tag': 'ORG', 'pageid': None, 'qid': None, 'title': None}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train shape: {df_train.shape}\")\n",
    "print(f\"Columns: {df_train.columns.tolist()}\")\n",
    "print(f\"\\nFirst row text length: {len(df_train.iloc[0]['text'])}\")\n",
    "print(f\"First row entities: {len(df_train.iloc[0]['entities'])}\")\n",
    "\n",
    "if len(df_train.iloc[0]['entities']) > 0:\n",
    "    print(f\"First entity: {df_train.iloc[0]['entities'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c42710c",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline\n",
    "\n",
    "The following steps transform the raw AIDA dataset into a format suitable for entity linking tasks. Each step addresses specific data quality issues and prepares the data for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114143ce",
   "metadata": {},
   "source": [
    "Apply preprocessing to a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49ed095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_train.head(100).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c339a39a",
   "metadata": {},
   "source": [
    "### Step 1: Filter Valid Entities (Optional)\n",
    "\n",
    "**Purpose:** Remove entities with invalid or incomplete information (e.g., missing QIDs, malformed spans).\n",
    "\n",
    "**What it does:**\n",
    "- Validates entity offsets are within text bounds\n",
    "- Ensures all required fields (start, end, mention, qid) are present\n",
    "- Filters out entities with corrupted data\n",
    "\n",
    "**When to use:** Enable if the dataset has quality issues. AIDA is generally clean, so this step is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e6f28d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sample = u.apply_filter_valid_entities(df_sample, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b64e7b",
   "metadata": {},
   "source": [
    "### Step 2: Add Context to Entities\n",
    "\n",
    "**Purpose:** Extract surrounding text context for each entity mention to provide disambiguation information.\n",
    "\n",
    "**What it does:**\n",
    "- Extracts text window before and after each mention (default: 200 characters)\n",
    "- Creates `context_left` and `context_right` fields\n",
    "- Preserves original text boundaries (doesn't cut mid-word)\n",
    "\n",
    "**Why it matters:** Context is crucial for entity linking - the surrounding words help determine which entity a mention refers to (e.g., \"Jordan the country\" vs \"Michael Jordan\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6845dac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding context (window=200) to entities...\n"
     ]
    }
   ],
   "source": [
    "df_sample = u.apply_add_context(df_sample, context_window=200, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c3caf0",
   "metadata": {},
   "source": [
    "### Step 3: Normalize Mentions\n",
    "\n",
    "**Purpose:** Standardize mention text for consistent matching and improved disambiguation.\n",
    "\n",
    "**What it does:**\n",
    "- Converts mentions to lowercase\n",
    "- Removes extra whitespace\n",
    "- Strips punctuation from edges\n",
    "- Creates a `normalized_mention` field\n",
    "\n",
    "**Why it matters:** \"Microsoft\", \"microsoft\", and \"MICROSOFT\" should be treated as the same mention. Normalization improves matching with knowledge bases and reduces vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "910fa102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing entity mentions...\n"
     ]
    }
   ],
   "source": [
    "df_sample = u.apply_normalize_mentions(df_sample, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9423a9",
   "metadata": {},
   "source": [
    "### Step 4: Remove Overlapping Entities\n",
    "\n",
    "**Purpose:** Resolve cases where multiple entity annotations cover the same or overlapping text spans.\n",
    "\n",
    "**What it does:**\n",
    "- Detects entities with overlapping character ranges\n",
    "- Keeps the longer/more specific entity when overlap occurs\n",
    "- Prevents duplicate or conflicting annotations\n",
    "\n",
    "**Example:** If \"New York City\" and \"York\" are both annotated, keeps \"New York City\" and removes the partial \"York\".\n",
    "\n",
    "**Why it matters:** Overlapping entities cause confusion during training and can lead to incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "396294ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing overlapping entities...\n",
      "  Removed 0 overlapping entities\n"
     ]
    }
   ],
   "source": [
    "df_sample = u.apply_remove_overlaps(df_sample, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fad163f",
   "metadata": {},
   "source": [
    "### Step 5: Create Candidate Pairs\n",
    "\n",
    "**Purpose:** Generate training examples by pairing each mention with its candidate entities (including the correct one).\n",
    "\n",
    "**What it does:**\n",
    "- For each mention, retrieves candidate entities from knowledge base\n",
    "- Creates (mention, candidate_entity) pairs\n",
    "- Includes context and features for ranking\n",
    "- Labels the correct entity as positive, others as negative\n",
    "\n",
    "**Output:** `mention_candidate_pairs` field containing list of candidates per mention.\n",
    "\n",
    "**Why it matters:** This creates the actual training data for the entity linking model - the model learns to rank candidates and select the correct entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcb23a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating mention-candidate pairs...\n"
     ]
    }
   ],
   "source": [
    "df_sample = u.apply_create_candidate_pairs(df_sample, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0254fc81",
   "metadata": {},
   "source": [
    "### Step 6: Create NIL Detection Examples\n",
    "\n",
    "**Purpose:** Generate examples for identifying mentions that don't link to any entity in the knowledge base (NIL entities).\n",
    "\n",
    "**What it does:**\n",
    "- Identifies mentions without valid entity links (qid is null)\n",
    "- Creates negative examples for NIL detection task\n",
    "- Separates NIL mentions from linkable mentions\n",
    "\n",
    "**Output:** `nil_examples` and `linked_examples` fields.\n",
    "\n",
    "**Why it matters:** Not all mentions can be linked to Wikipedia/Wikidata. The model needs to learn when to predict \"no entity exists\" rather than forcing a wrong link. This is crucial for real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09a29bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating NIL detection examples...\n",
      "  NIL entities: 504, Linked entities: 2139\n"
     ]
    }
   ],
   "source": [
    "df_sample = u.apply_create_nil_examples(df_sample, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15aa076",
   "metadata": {},
   "source": [
    "### Step 7: Split Long Documents (Optional)\n",
    "\n",
    "**Purpose:** Break very long documents into smaller chunks that fit model input limits.\n",
    "\n",
    "**What it does:**\n",
    "- Splits documents exceeding max_length (e.g., 512 tokens)\n",
    "- Preserves entity annotations within each chunk\n",
    "- Maintains context continuity where possible\n",
    "\n",
    "**When to use:** Enable when using transformer models with fixed input limits (BERT, RoBERTa, etc.).\n",
    "\n",
    "**Note:** Currently disabled for AIDA as documents are reasonably sized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbd18f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Processed shape: {df_sample.shape}\")\n",
    "print(f\"New columns: {df_sample.columns.tolist()}\")\n",
    "print(f\"\\nSample of new fields:\")\n",
    "print(f\"- Context fields: context_left, context_right\")\n",
    "print(f\"- Normalized mentions: normalized_mention\")\n",
    "print(f\"- Candidate pairs: mention_candidate_pairs\")\n",
    "print(f\"- NIL detection: nil_examples, linked_examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d0136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if documents exceed model input limits\n",
    "# df_sample = u.apply_split_long_documents(df_sample, max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49249171",
   "metadata": {},
   "source": [
    "### Export Preprocessed Data\n",
    "\n",
    "Save the preprocessed sample for inspection or downstream use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63959d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.to_parquet(\"preprocessed_sample_aida.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5300efee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: View structure of candidate pairs\n",
    "# x = df_sample[['mention_candidate_pairs', 'nil_examples', 'linked_examples']].head(1)\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4b995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Expand candidate pairs to see individual entries\n",
    "# df_mcp = x['mention_candidate_pairs'].explode().apply(pd.Series)\n",
    "# df_mcp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
