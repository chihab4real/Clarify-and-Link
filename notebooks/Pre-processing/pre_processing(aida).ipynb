{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58bbb387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e479fc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "\n",
    "\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "import Utils as u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3851bc",
   "metadata": {},
   "source": [
    "Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67df7a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"hf://datasets/cyanic-selkie/aida-conll-yago-wikidata/\"\n",
    "splits = {'train': 'train.parquet', 'validation': 'validation.parquet', 'test': 'test.parquet'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcdaf814",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_parquet(base_url + splits['train'])\n",
    "df_val = pd.read_parquet(base_url + splits['validation'])\n",
    "df_test = pd.read_parquet(base_url + splits['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e987b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 946 examples\n",
      "Validation: 216 examples\n",
      "Test: 231 examples\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train: {len(df_train)} examples\")\n",
    "print(f\"Validation: {len(df_val)} examples\")\n",
    "print(f\"Test: {len(df_test)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c873a4",
   "metadata": {},
   "source": [
    "Showing Original Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bc7175c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (946, 3)\n",
      "Columns: ['document_id', 'text', 'entities']\n",
      "\n",
      "First row text length: 2790\n",
      "First row entities: 48\n",
      "First entity: {'start': 0, 'end': 2, 'tag': 'ORG', 'pageid': None, 'qid': None, 'title': None}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train shape: {df_train.shape}\")\n",
    "print(f\"Columns: {df_train.columns.tolist()}\")\n",
    "print(f\"\\nFirst row text length: {len(df_train.iloc[0]['text'])}\")\n",
    "print(f\"First row entities: {len(df_train.iloc[0]['entities'])}\")\n",
    "\n",
    "if len(df_train.iloc[0]['entities']) > 0:\n",
    "    print(f\"First entity: {df_train.iloc[0]['entities'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c42710c",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline\n",
    "\n",
    "The following steps transform the raw AIDA dataset into a format suitable for entity linking tasks. Each step addresses specific data quality issues and prepares the data for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114143ce",
   "metadata": {},
   "source": [
    "Apply preprocessing to a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49ed095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sample = df_train.head(100).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c339a39a",
   "metadata": {},
   "source": [
    "### Step 1: Filter Valid Entities (Optional)\n",
    "\n",
    "**Purpose:** Remove entities with invalid or incomplete information (e.g., missing QIDs, malformed spans).\n",
    "\n",
    "**What it does:**\n",
    "- Validates entity offsets are within text bounds\n",
    "- Ensures all required fields (start, end, mention, qid) are present\n",
    "- Filters out entities with corrupted data\n",
    "\n",
    "**When to use:** Enable if the dataset has quality issues. AIDA is generally clean, so this step is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e6f28d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sample = u.apply_filter_valid_entities(df_sample, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b64e7b",
   "metadata": {},
   "source": [
    "### Step 2: Add Context to Entities\n",
    "\n",
    "**Purpose:** Extract surrounding text context for each entity mention to provide disambiguation information.\n",
    "\n",
    "**What it does:**\n",
    "- Extracts text window before and after each mention (default: 200 characters)\n",
    "- Creates `context_left` and `context_right` fields\n",
    "- Preserves original text boundaries (doesn't cut mid-word)\n",
    "\n",
    "**Why it matters:** Context is crucial for entity linking - the surrounding words help determine which entity a mention refers to (e.g., \"Jordan the country\" vs \"Michael Jordan\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6845dac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sample = u.apply_add_context(df_sample, context_window=200, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42040464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding context (window=200) to entities...\n",
      "Adding context (window=200) to entities...\n",
      "Adding context (window=200) to entities...\n"
     ]
    }
   ],
   "source": [
    "df_train_processed = u.apply_add_context(df_train, context_window=200, inplace=False)\n",
    "df_val_processed = u.apply_add_context(df_val, context_window=200, inplace=False)\n",
    "df_test_processed = u.apply_add_context(df_test, context_window=200, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c3caf0",
   "metadata": {},
   "source": [
    "### Step 3: Normalize Mentions\n",
    "\n",
    "**Purpose:** Standardize mention text for consistent matching and improved disambiguation.\n",
    "\n",
    "**What it does:**\n",
    "- Converts mentions to lowercase\n",
    "- Removes extra whitespace\n",
    "- Strips punctuation from edges\n",
    "- Creates a `normalized_mention` field\n",
    "\n",
    "**Why it matters:** \"Microsoft\", \"microsoft\", and \"MICROSOFT\" should be treated as the same mention. Normalization improves matching with knowledge bases and reduces vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "910fa102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sample = u.apply_normalize_mentions(df_sample, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81cd06a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing mentions in 946 documents...\n",
      "Normalizing mentions in 216 documents...\n",
      "Normalizing mentions in 231 documents...\n"
     ]
    }
   ],
   "source": [
    "df_train_processed = u.apply_normalize_mentions(df_train_processed, inplace=True)\n",
    "df_val_processed = u.apply_normalize_mentions(df_val_processed, inplace=True)\n",
    "df_test_processed = u.apply_normalize_mentions(df_test_processed, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9423a9",
   "metadata": {},
   "source": [
    "### Step 4: Remove Overlapping Entities\n",
    "\n",
    "**Purpose:** Resolve cases where multiple entity annotations cover the same or overlapping text spans.\n",
    "\n",
    "**What it does:**\n",
    "- Detects entities with overlapping character ranges\n",
    "- Keeps the longer/more specific entity when overlap occurs\n",
    "- Prevents duplicate or conflicting annotations\n",
    "\n",
    "**Example:** If \"New York City\" and \"York\" are both annotated, keeps \"New York City\" and removes the partial \"York\".\n",
    "\n",
    "**Why it matters:** Overlapping entities cause confusion during training and can lead to incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "396294ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sample = u.apply_remove_overlaps(df_sample, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebb64e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing overlapping entities...\n",
      "  Removed 0 overlapping entities\n",
      "Removing overlapping entities...\n",
      "  Removed 0 overlapping entities\n",
      "Removing overlapping entities...\n",
      "  Removed 0 overlapping entities\n"
     ]
    }
   ],
   "source": [
    "df_train_processed = u.apply_remove_overlaps(df_train_processed, inplace=True)\n",
    "df_val_processed = u.apply_remove_overlaps(df_val_processed, inplace=True)\n",
    "df_test_processed = u.apply_remove_overlaps(df_test_processed, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2810b042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Saved processed data to ../../data/processed/aida/\n",
      "   Train: 946 docs\n",
      "   Val: 216 docs\n",
      "   Test: 231 docs\n",
      "\n",
      "üîç Verifying preprocessing...\n",
      "Sample entity keys: ['start', 'end', 'tag', 'pageid', 'qid', 'title', 'mention', 'left_context', 'right_context', 'full_context', 'mention_start', 'mention_end', 'normalized_mention']\n",
      "‚úÖ normalized_mention field present!\n",
      "‚úÖ Validation unique normalized mentions: 2597\n",
      "   (Should be ~1,500-2,000 instead of 2,795)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output_dir = '../../data/processed/aida'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "df_train_processed.to_parquet(f'{output_dir}/train.parquet', index=False)\n",
    "df_val_processed.to_parquet(f'{output_dir}/validation.parquet', index=False)\n",
    "df_test_processed.to_parquet(f'{output_dir}/test.parquet', index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved processed data to {output_dir}/\")\n",
    "print(f\"   Train: {len(df_train_processed)} docs\")\n",
    "print(f\"   Val: {len(df_val_processed)} docs\")\n",
    "print(f\"   Test: {len(df_test_processed)} docs\")\n",
    "\n",
    "# Verify normalized_mention exists\n",
    "print(\"\\nüîç Verifying preprocessing...\")\n",
    "sample = df_val_processed.iloc[0]['entities'][0]\n",
    "print(f\"Sample entity keys: {list(sample.keys())}\")\n",
    "\n",
    "if 'normalized_mention' in sample:\n",
    "    print(\"‚úÖ normalized_mention field present!\")\n",
    "    \n",
    "    # Count unique normalized mentions in validation\n",
    "    unique_val = set()\n",
    "    for _, row in df_val_processed.iterrows():\n",
    "        for entity in row['entities']:\n",
    "            unique_val.add(entity['normalized_mention'])\n",
    "    \n",
    "    print(f\"‚úÖ Validation unique normalized mentions: {len(unique_val)}\")\n",
    "    print(f\"   (Should be ~1,500-2,000 instead of 2,795)\")\n",
    "else:\n",
    "    print(\"‚ùå WARNING: normalized_mention missing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ce648a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Checking TRAIN set deduplication...\n",
      "üìä Train total entities: 23,393\n",
      "üìä Train unique normalized: 7,542\n",
      "üìä Train reduction: 67.8%\n",
      "‚è±Ô∏è Estimated clarification time: 20.9 minutes (parallel, 3 workers)\n"
     ]
    }
   ],
   "source": [
    "# Check train set reduction\n",
    "print(\"\\nüîç Checking TRAIN set deduplication...\")\n",
    "\n",
    "train_normalized = set()\n",
    "train_total = 0\n",
    "for _, row in df_train_processed.iterrows():\n",
    "    for entity in row['entities']:\n",
    "        train_normalized.add(entity.get('normalized_mention', ''))\n",
    "        train_total += 1\n",
    "\n",
    "print(f\"üìä Train total entities: {train_total:,}\")\n",
    "print(f\"üìä Train unique normalized: {len(train_normalized):,}\")\n",
    "print(f\"üìä Train reduction: {((train_total - len(train_normalized)) / train_total * 100):.1f}%\")\n",
    "print(f\"‚è±Ô∏è Estimated clarification time: {len(train_normalized) * 0.5 / 3 / 60:.1f} minutes (parallel, 3 workers)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
