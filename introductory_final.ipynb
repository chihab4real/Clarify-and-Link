{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4758d284",
   "metadata": {},
   "source": [
    "# Introduction Notebook\n",
    "\n",
    "This notebook covers the main tools that will be used during our project with an example usage.\n",
    "\n",
    "1. LLM interaction (NER + Clarification)\n",
    "2. Sequence updating with entities + clarifications\n",
    "3. Prepare dataset for T5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13f6b289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f352bb46",
   "metadata": {},
   "source": [
    "## 1. LLM interaction (NER + Clarification)\n",
    "- in the original paper there was use model **llama3:70b**\n",
    "- for showing the usage of the tool there will be present of example on model **llama3.1:latest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58e08739",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 667b0c1932bc: 100% ▕██████████████████▏ 4.9 GB                         \u001b[K\n",
      "pulling 948af2743fc7: 100% ▕██████████████████▏ 1.5 KB                         \u001b[K\n",
      "pulling 0ba8f0e314b4: 100% ▕██████████████████▏  12 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 455f34728c9b: 100% ▕██████████████████▏  487 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME               ID              SIZE      MODIFIED               \n",
      "llama3.1:latest    46e0c10c039e    4.9 GB    Less than a second ago    \n"
     ]
    }
   ],
   "source": [
    "# 1 Step - install Ollama from the website https://ollama.com\n",
    "\n",
    "# 2 Step - install a model from the terminal\n",
    "!ollama pull llama3.1:latest\n",
    "\n",
    "# 3 Step - check the correctness of installation of specific model\n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef89618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_ollama(sentence: str, model: str = \"llama3.1:latest\") -> list:\n",
    "    \n",
    "    \"\"\"Query LLM to extract entities in JSON format (as list).\"\"\"\n",
    "\n",
    "    request = (\n",
    "        \"Please generate one list with all entities from the following text \"\n",
    "        \"in JSON format, excluding numbers. Do not format the JSON output. \"\n",
    "        + sentence\n",
    "    )\n",
    "    response = ollama.chat(model=model, messages=[{\"role\": \"user\", \"content\": request}])\n",
    "    output = response['message']['content']\n",
    "    # Extract JSON array\n",
    "    match = re.search(r'\\[.*?\\]', output, re.DOTALL)\n",
    "    if match:\n",
    "        try:\n",
    "            entities = json.loads(match.group(0))\n",
    "            return entities\n",
    "        except json.JSONDecodeError:\n",
    "            return []\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0626b731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clarify_entity(entity: str, context: str, model: str = \"llama3.1:latest\") -> str:\n",
    "    \n",
    "    \"\"\"Ask LLM to provide a 2-3 sentence description for the entity based on context.\"\"\"\n",
    "\n",
    "    request = (\n",
    "        f\"Just expand the following entity mention '{entity}' to a description \"\n",
    "        f\"(2-3 sentences) based on context. Context: {context}\"\n",
    "    )\n",
    "    response = ollama.chat(model=model, messages=[{\"role\": \"user\", \"content\": request}])\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f76407",
   "metadata": {},
   "source": [
    "## 2. Sequence updating with entities + clarifications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee3fdfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_sequence_with_entities(sequence: str, clarify_entities: dict) -> str:\n",
    "    \"\"\"Insert [START_ENT]... [END_ENT] for all entities.\"\"\"\n",
    "    if not clarify_entities:\n",
    "        return sequence\n",
    "\n",
    "    escaped_keys = [re.escape(k) for k in clarify_entities.keys()]\n",
    "    pattern = re.compile(r'\\b(' + \"|\".join(escaped_keys) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "    def repl(match):\n",
    "        entity_text = match.group(0)\n",
    "        return f\"[START_ENT] {entity_text} [END_ENT]\"\n",
    "\n",
    "    return pattern.sub(repl, sequence)\n",
    "\n",
    "def update_sequence_with_entities_clarify(sequence: str, clarify_entities: dict) -> str:\n",
    "    \"\"\"Insert [START_ENT]... [END_ENT][CLARIFY: ...] for all entities.\"\"\"\n",
    "    if not clarify_entities:\n",
    "        return sequence\n",
    "\n",
    "    escaped_keys = [re.escape(k) for k in clarify_entities.keys()]\n",
    "    pattern = re.compile(r'\\b(' + \"|\".join(escaped_keys) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "    def repl(match):\n",
    "        entity_text = match.group(0)\n",
    "        # find the key in original dict (case-insensitive)\n",
    "        real_key = next(k for k in clarify_entities if k.lower() == entity_text.lower())\n",
    "        return f\"[START_ENT] {entity_text} [END_ENT][CLARIFY: {clarify_entities[real_key]}]\"\n",
    "\n",
    "    return pattern.sub(repl, sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec50ab8",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5c18c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted entities: ['Angelina', 'Brad', 'Jon', 'AK']\n",
      "Clarifications: {'Angelina': 'Here\\'s an expanded description of \"Angelina\" based on the provided context:\\n\\nAngelina Jolie, humanitarian and philanthropist, met her longtime partner, actor Brad Pitt, while introducing herself as a mother of six to the press. Her father, Jon Voight, being a pastor himself, had likely instilled in her strong Christian values from an early age. As a well-known actress and activist, Angelina\\'s compassionate nature has led her to advocate for women\\'s rights and refugee causes around the world.', 'Brad': 'Here\\'s an expanded description of \"Brad\" based on the provided context:\\n\\nBrad, presumably Angelina\\'s partner, appears to be a significant figure in her life, as she has likely introduced him to her family. As he accompanies Angelina to meet her father, it suggests that their relationship is established and possibly even familial, given the informal introduction.', 'Jon': 'Here\\'s an expanded description of \"Jon\" based on the given context:\\n\\nJon is a devout man, serving as a respected pastor in Anchorage, Alaska (AK). He likely values faith and community, being a spiritual leader to many in his congregation. His influence can be seen in Angelina\\'s upbringing, which may have instilled her with strong moral principles and compassion.', 'AK': 'Based on the context, I\\'ll expand the entity mention \"AK\" to:\\n\\nAlaska.\\n\\nHere\\'s the expanded paragraph with a brief description:\\nAngelina met her partner Brad and her father Jon, who was a pastor at their church in Alaska. The vast, rugged landscape of Alaska provided a unique backdrop for their family gatherings. As a pastor, Jon often used the natural beauty of Alaska as inspiration for his sermons.'}\n",
      "Augmented text: [START_ENT] Angelina [END_ENT] met her partner [START_ENT] Brad [END_ENT] and her father [START_ENT] Jon [END_ENT] which is a pastor in [START_ENT] AK [END_ENT] \n",
      "Augmented text: [START_ENT] Angelina [END_ENT][CLARIFY: Here's an expanded description of \"Angelina\" based on the provided context:\n",
      "\n",
      "Angelina Jolie, humanitarian and philanthropist, met her longtime partner, actor Brad Pitt, while introducing herself as a mother of six to the press. Her father, Jon Voight, being a pastor himself, had likely instilled in her strong Christian values from an early age. As a well-known actress and activist, Angelina's compassionate nature has led her to advocate for women's rights and refugee causes around the world.] met her partner [START_ENT] Brad [END_ENT][CLARIFY: Here's an expanded description of \"Brad\" based on the provided context:\n",
      "\n",
      "Brad, presumably Angelina's partner, appears to be a significant figure in her life, as she has likely introduced him to her family. As he accompanies Angelina to meet her father, it suggests that their relationship is established and possibly even familial, given the informal introduction.] and her father [START_ENT] Jon [END_ENT][CLARIFY: Here's an expanded description of \"Jon\" based on the given context:\n",
      "\n",
      "Jon is a devout man, serving as a respected pastor in Anchorage, Alaska (AK). He likely values faith and community, being a spiritual leader to many in his congregation. His influence can be seen in Angelina's upbringing, which may have instilled her with strong moral principles and compassion.] which is a pastor in [START_ENT] AK [END_ENT][CLARIFY: Based on the context, I'll expand the entity mention \"AK\" to:\n",
      "\n",
      "Alaska.\n",
      "\n",
      "Here's the expanded paragraph with a brief description:\n",
      "Angelina met her partner Brad and her father Jon, who was a pastor at their church in Alaska. The vast, rugged landscape of Alaska provided a unique backdrop for their family gatherings. As a pastor, Jon often used the natural beauty of Alaska as inspiration for his sermons.] \n"
     ]
    }
   ],
   "source": [
    "text = \"Angelina met her partner Brad and her father Jon which is a pastor in AK \"\n",
    "\n",
    "# Step 1: extract entities\n",
    "entities = query_ollama(text)\n",
    "entities= ['Angelina', 'Brad', 'Jon', 'AK']  # for demonstration purposes\n",
    "print(\"Extracted entities:\", entities)\n",
    "\n",
    "# Step 2: clarify each entity\n",
    "clarify_entities_dict = {entity: clarify_entity(entity, text) for entity in entities}\n",
    "print(\"Clarifications:\", clarify_entities_dict)\n",
    "\n",
    "# Step 3: update sequence with entities\n",
    "augmented_text = update_sequence_with_entities(text, clarify_entities_dict)\n",
    "print(\"Augmented text:\", augmented_text)\n",
    "\n",
    "# Step 3: update sequence with clarifications\n",
    "clarify_augmented_text = update_sequence_with_entities_clarify(text, clarify_entities_dict)\n",
    "print(\"Augmented text:\", clarify_augmented_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8946ab28",
   "metadata": {},
   "source": [
    "## 3. Prepare dataset for T5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "685c2ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class JointDataset(Dataset):\n",
    "    def __init__(self, samples, tokenizer, max_length=128):\n",
    "        self.samples = samples\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            sample[\"input_text\"],\n",
    "            text_target=sample[\"target_text\"],\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # Flatten tensors\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9d4f389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "c:\\Users\\joasi\\Documents\\PP\\SEM_7\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 01:55, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.896200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>13.108000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7.679600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>10.753700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6.518500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>9.094700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>9.658300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>5.773300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>7.968200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.707000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>5.803800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>6.004300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>5.620900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>5.417200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>4.692200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.294900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>4.199400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.302300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>6.056200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.846400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.288400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.906100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.074700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>5.799600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.672800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>4.462800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.913500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>3.702000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.712800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>4.502600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.438900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>4.142400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.813200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>3.530700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.469500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>5.365200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2.308600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>3.604800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.478800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.316300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>3.801500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.086300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>2.736400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.525000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>4.956300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.267700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>2.837000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.701700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>3.011000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.846800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.339800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>2.945100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.830900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>3.074000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>3.730200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>4.024700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.370300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.205800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.008600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>2.096400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.244100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>3.114300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.365600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>2.701000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.093400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>2.697200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.615800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.012700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.673400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.898900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>3.807200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.434200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>5.557500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.956500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>2.660200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.903600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>2.364900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>1.192100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.468700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>1.196900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>2.636700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>2.700300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.539400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>2.613200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.958300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>2.080800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.776400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>2.537400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.808300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>2.425500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.768300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>2.553600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.618700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.316500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>3.125600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>1.290900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>2.447000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>2.391100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.851800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joasi\\Documents\\PP\\SEM_7\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\joasi\\Documents\\PP\\SEM_7\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\joasi\\Documents\\PP\\SEM_7\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\joasi\\Documents\\PP\\SEM_7\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\joasi\\Documents\\PP\\SEM_7\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\joasi\\Documents\\PP\\SEM_7\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\joasi\\Documents\\PP\\SEM_7\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\joasi\\Documents\\PP\\SEM_7\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\joasi\\Documents\\PP\\SEM_7\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=3.1856230801343917, metrics={'train_runtime': 117.1267, 'train_samples_per_second': 0.854, 'train_steps_per_second': 0.854, 'total_flos': 3383545036800.0, 'train_loss': 3.1856230801343917, 'epoch': 50.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples = [\n",
    "    # NER task: input is plain text + target_ner\n",
    "    {\n",
    "        \"input_text\": \"Angelina met Brad in AK. target_ner\",\n",
    "        \"target_text\": \"[START_ENT] Angelina [END_ENT][CLARIFY: An actress] met [START_ENT] Brad [END_ENT][CLARIFY: Brad Pitt is Angelina's former partner, whom she married from 2014 to 2019] in [START_ENT] AK [END_ENT][CLARIFY: Alaska]\"\n",
    "    },\n",
    "    # EL task: input is annotated text + target_el\n",
    "    {\n",
    "        \"input_text\": \"[START_ENT] Angelina [END_ENT][CLARIFY: An actress] met [START_ENT] Brad [END_ENT][CLARIFY: Brad Pitt is Angelina's former partner, whom she married from 2014 to 2019] in [START_ENT] AK [END_ENT][CLARIFY: Alaska] target.el\",\n",
    "        \"target_text\": \"Angelina Jolie met Brad Pitt in Alaska\"\n",
    "    }\n",
    "]\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "dataset = JointDataset(train_samples, tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5_joint_demo\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=1,\n",
    "    logging_steps=1,\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    learning_rate=5e-5,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a95eea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Output: Angerina Angelina met Jon Jon in AK\n",
      "EL Output: AK  AK (AK)\n"
     ]
    }
   ],
   "source": [
    "test_sentence_ner = \"Angelina met Jon in AK. target_ner\"\n",
    "inputs = tokenizer(test_sentence_ner, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=128)\n",
    "print(\"NER Output:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "test_sentence_el = \"[START_ENT] Angelina [END_ENT][CLARIFY: An actress] met [START_ENT] Brad [END_ENT][CLARIFY: Brad Pitt is Angelina's former partner, whom she married from 2014 to 2019] in [START_ENT] AK [END_ENT][CLARIFY: Alaska] target.el\"\n",
    "inputs = tokenizer(test_sentence_el, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=128)\n",
    "print(\"EL Output:\", tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a7b45b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623e39bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
