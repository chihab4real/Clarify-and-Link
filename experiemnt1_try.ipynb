{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RUN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DZo0F55wAAd",
        "outputId": "b653c4ad-dc1e-40fa-b8af-9deb2dcd4a8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing dependencies...\n",
            "Installation complete!\n",
            "\n",
            "  Device: CPU\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Cell 1: Setup and Installation\n",
        "# ============================================================================\n",
        "\n",
        "print(\"Installing dependencies...\")\n",
        "\n",
        "#pip install -q transformers torch pandas pyarrow tqdm accelerate\n",
        "\n",
        "print(\"Installation complete!\")\n",
        "\n",
        "import torch\n",
        "print(f\"\\n  Device: {'GPU (' + torch.cuda.get_device_name(0) + ')' if torch.cuda.is_available() else 'CPU'}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\" GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "zNcqpi_hwttH",
        "outputId": "b146f590-d4a0-40c9-d028-6337f6b33736"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upload your AIDA data files:\n",
            "   Required: train.parquet, validation.parquet, test.parquet\n",
            "\n",
            "   Click 'Choose Files' and select all 3 files\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b3fc7273-6487-4afd-a85d-68d6e5ba9cef\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b3fc7273-6487-4afd-a85d-68d6e5ba9cef\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Cell 2: Upload AIDA Data Files\n",
        "# ============================================================================\n",
        "\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "print(\"Upload your AIDA data files:\")\n",
        "print(\"   Required: train.parquet, validation.parquet, test.parquet\")\n",
        "print(\"\\n   Click 'Choose Files' and select all 3 files\")\n",
        "\n",
        "os.makedirs('data/processed/aida', exist_ok=True)\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    os.rename(filename, f'data/processed/aida/{filename}')\n",
        "    print(f\"‚úì {filename} uploaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RUN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9VpOhHTw1OJ",
        "outputId": "41586b4d-0726-4188-878a-4e4c10465848"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration:\n",
            "   model_name: meta-llama/Llama-3.2-1B\n",
            "   batch_size: 32\n",
            "   max_new_tokens: 50\n",
            "   temperature: 0.3\n",
            "   context_window_size: 100\n",
            "   data_dir: data/processed/aida\n",
            "   output_dir: data/experiments\n",
            "   checkpoint_interval: 500\n",
            "   device: cpu\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Cell 3: Configuration\n",
        "# ============================================================================\n",
        "\n",
        "CONFIG = {\n",
        "    'model_name': 'meta-llama/Llama-3.2-1B',  # Lightweight model for Colab\n",
        "    ##'model_name': 'facebook/opt-1.3b',\n",
        "    'batch_size': 32,  # Increase for faster processing on GPU\n",
        "    'max_new_tokens': 50,\n",
        "    'temperature': 0.3,\n",
        "    'context_window_size': 100,\n",
        "    'data_dir': 'data/processed/aida',\n",
        "    'output_dir': 'data/experiments',\n",
        "    'checkpoint_interval': 500,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "}\n",
        "\n",
        "print(\"Configuration:\")\n",
        "for key, value in CONFIG.items():\n",
        "    print(f\"   {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RUN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyoqLO3BzX7A",
        "outputId": "78bb7193-f4fc-4f36-ec79-95a730e1e014"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authenticated with HuggingFace!\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "token = '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'\n",
        "login(token=token)\n",
        "\n",
        "print(\"Authenticated with HuggingFace!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RUN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypBvo8T5xam0",
        "outputId": "140d1d6e-8486-443b-9898-9c60fedc1b3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "LOADING MODEL\n",
            "======================================================================\n",
            "\n",
            " Model: meta-llama/Llama-3.2-1B\n",
            "  Device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Model loaded on cpu\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Cell 4: Load Model and Tokenizer\n",
        "# ============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "def load_model_and_tokenizer():\n",
        "    \"\"\"Load HuggingFace model once and keep in memory.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"LOADING MODEL\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\n Model: {CONFIG['model_name']}\")\n",
        "    print(f\"  Device: {CONFIG['device']}\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        CONFIG['model_name'],\n",
        "        torch_dtype=torch.float16 if CONFIG['device'] == 'cuda' else torch.float32,\n",
        "        device_map='auto'\n",
        "    )\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"‚úì Model loaded on {CONFIG['device']}\")\n",
        "    if CONFIG['device'] == 'cuda':\n",
        "        print(f\"‚úì GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB allocated\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "model, tokenizer = load_model_and_tokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4L3nQiPzy2F",
        "outputId": "0cd45cac-79dc-4830-f0eb-2c9039007671"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Helper functions loaded!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Cell 5: Helper Functions\n",
        "# ============================================================================\n",
        "\n",
        "def create_prompt(mention, context_left, context_right):\n",
        "    \"\"\"Create clarification prompt.\"\"\"\n",
        "    window_size = CONFIG['context_window_size']\n",
        "\n",
        "    context_left = context_left[-window_size:] if len(context_left) > window_size else context_left\n",
        "    context_right = context_right[:window_size] if len(context_right) > window_size else context_right\n",
        "\n",
        "    prompt = f\"\"\"Based on this context: \"{context_left} {mention} {context_right}\"\n",
        "\n",
        "Provide a brief, factual description for the entity \"{mention}\".\n",
        "Identify what this specific mention refers to.\n",
        "Use simple English (max 40 words).\n",
        "\n",
        "Description:\"\"\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def generate_clarifications_batch(model, tokenizer, batch_data):\n",
        "    \"\"\"\n",
        "    Generate clarifications for a batch of mentions.\n",
        "\n",
        "    Args:\n",
        "        batch_data: List of (mention, context_left, context_right, normalized) tuples\n",
        "\n",
        "    Returns:\n",
        "        List of clarifications\n",
        "    \"\"\"\n",
        "    # Create prompts\n",
        "    prompts = [\n",
        "        create_prompt(mention, ctx_left, ctx_right)\n",
        "        for mention, ctx_left, ctx_right, _ in batch_data\n",
        "    ]\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        prompts,\n",
        "        return_tensors='pt',\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    ).to(CONFIG['device'])\n",
        "\n",
        "    # Generate batch\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=CONFIG['max_new_tokens'],\n",
        "            temperature=CONFIG['temperature'],\n",
        "            do_sample=False,  # Deterministic\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode batch\n",
        "    clarifications = []\n",
        "    for i, output in enumerate(outputs):\n",
        "        # Remove prompt from output\n",
        "        prompt_length = inputs['input_ids'][i].shape[0]\n",
        "        generated_ids = output[prompt_length:]\n",
        "\n",
        "        clarification = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "        # Fallback if empty\n",
        "        if not clarification:\n",
        "            clarification = f\"Entity: {batch_data[i][0]}\"\n",
        "\n",
        "        clarifications.append(clarification)\n",
        "\n",
        "    return clarifications\n",
        "\n",
        "\n",
        "def load_aida_data():\n",
        "    \"\"\"Load preprocessed AIDA train/val/test splits.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"LOADING AIDA DATA\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    data_dir = CONFIG['data_dir']\n",
        "    print(f\"\\n Loading from: {data_dir}\")\n",
        "\n",
        "    df_train = pd.read_parquet(f'{data_dir}/train.parquet')\n",
        "    df_val = pd.read_parquet(f'{data_dir}/validation.parquet')\n",
        "    df_test = pd.read_parquet(f'{data_dir}/test.parquet')\n",
        "\n",
        "    print(f\"\\n‚úì Train: {len(df_train)} documents\")\n",
        "    print(f\"‚úì Validation: {len(df_val)} documents\")\n",
        "    print(f\"‚úì Test: {len(df_test)} documents\")\n",
        "\n",
        "    # Count entities\n",
        "    train_entities = sum(len(row['entities']) for _, row in df_train.iterrows())\n",
        "    val_entities = sum(len(row['entities']) for _, row in df_val.iterrows())\n",
        "    test_entities = sum(len(row['entities']) for _, row in df_test.iterrows())\n",
        "\n",
        "    print(f\"\\n Total entities:\")\n",
        "    print(f\"   Train: {train_entities:,}\")\n",
        "    print(f\"   Val: {val_entities:,}\")\n",
        "    print(f\"   Test: {test_entities:,}\")\n",
        "\n",
        "    return df_train, df_val, df_test\n",
        "\n",
        "\n",
        "def collect_unique_mentions(df, split_name):\n",
        "    \"\"\"Collect all unique normalized mentions.\"\"\"\n",
        "    print(f\"\\nüîç Collecting unique mentions from {split_name}...\")\n",
        "\n",
        "    unique_mentions = {}\n",
        "    original_case_map = {}\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        for entity in row['entities']:\n",
        "            normalized = entity.get('normalized_mention', entity.get('mention', '').lower().strip())\n",
        "            original = entity.get('mention', '')\n",
        "\n",
        "            if normalized not in unique_mentions:\n",
        "                unique_mentions[normalized] = {\n",
        "                    'context_left': entity.get('context_left', ''),\n",
        "                    'context_right': entity.get('context_right', '')\n",
        "                }\n",
        "                original_case_map[normalized] = original\n",
        "\n",
        "    total_entities = sum(len(row['entities']) for _, row in df.iterrows())\n",
        "    reduction = (1 - len(unique_mentions)/total_entities) * 100\n",
        "\n",
        "    print(f\"   Unique mentions: {len(unique_mentions)} (vs {total_entities} total)\")\n",
        "    print(f\"   Reduction: {reduction:.1f}%\")\n",
        "\n",
        "    return unique_mentions, original_case_map\n",
        "\n",
        "print(\" Helper functions loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbX_ZONFxpKT",
        "outputId": "e2857c12-e9ec-4ecd-c806-32a93178fdb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Main generation function loaded (FIXED)!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Cell 6: Main Generation Function (FIXED)\n",
        "# ============================================================================\n",
        "\n",
        "def convert_to_serializable(obj):\n",
        "    \"\"\"Convert numpy arrays and other non-serializable objects to Python types.\"\"\"\n",
        "    import numpy as np\n",
        "\n",
        "    if isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    elif isinstance(obj, np.integer):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, np.floating):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, dict):\n",
        "        return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [convert_to_serializable(item) for item in obj]\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "\n",
        "def generate_clarifications_for_split(model, tokenizer, df, split_name):\n",
        "    \"\"\"Generate clarifications for entire split using batching.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"PROCESSING: {split_name.upper()}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    checkpoint_dir = f\"{CONFIG['output_dir']}/clarifications_checkpoints/{split_name}\"\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Collect unique mentions\n",
        "    unique_mentions, original_case_map = collect_unique_mentions(df, split_name)\n",
        "\n",
        "    # Prepare batch data\n",
        "    batch_data = [\n",
        "        (original_case_map[norm], context['context_left'], context['context_right'], norm)\n",
        "        for norm, context in unique_mentions.items()\n",
        "    ]\n",
        "\n",
        "    # Estimate time\n",
        "    num_batches = len(batch_data) // CONFIG['batch_size'] + 1\n",
        "    estimated_time = num_batches * 0.5 / 60  # ~0.5s per batch\n",
        "    print(f\"\\n Batched generation:\")\n",
        "    print(f\"   Batch size: {CONFIG['batch_size']}\")\n",
        "    print(f\"   Total batches: {num_batches}\")\n",
        "    print(f\"   Estimated time: {estimated_time:.1f} minutes\")\n",
        "\n",
        "    # Generate in batches\n",
        "    global_clarifications = {}\n",
        "\n",
        "    for i in tqdm(range(0, len(batch_data), CONFIG['batch_size']), desc=\"Generating batches\"):\n",
        "        batch = batch_data[i:i + CONFIG['batch_size']]\n",
        "\n",
        "        clarifications = generate_clarifications_batch(model, tokenizer, batch)\n",
        "\n",
        "        # Store results\n",
        "        for (mention, _, _, normalized), clarification in zip(batch, clarifications):\n",
        "            global_clarifications[normalized] = clarification\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (i // CONFIG['batch_size'] + 1) % (CONFIG['checkpoint_interval'] // CONFIG['batch_size']) == 0:\n",
        "            checkpoint_path = f'{checkpoint_dir}/checkpoint_{i + len(batch)}.json'\n",
        "            with open(checkpoint_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(global_clarifications, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # Map to documents\n",
        "    print(f\"\\n Mapping clarifications to documents...\")\n",
        "    results = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        doc_clarifications = {}\n",
        "        for entity in row['entities']:\n",
        "            original_mention = entity['mention']\n",
        "            normalized = entity.get('normalized_mention', original_mention.lower().strip())\n",
        "            doc_clarifications[original_mention] = global_clarifications.get(\n",
        "                normalized,\n",
        "                f\"Entity: {original_mention}\"\n",
        "            )\n",
        "\n",
        "        # Convert entities to serializable format (FIX HERE)\n",
        "        serializable_entities = [convert_to_serializable(entity) for entity in row['entities']]\n",
        "\n",
        "        results.append({\n",
        "            'doc_id': int(idx),  # Convert to int\n",
        "            'text': str(row['text']),  # Ensure string\n",
        "            'entities': serializable_entities,\n",
        "            'clarifications': doc_clarifications\n",
        "        })\n",
        "\n",
        "    # Save final\n",
        "    output_path = f\"{CONFIG['output_dir']}/clarifications_{split_name}.json\"\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\n {split_name.upper()} complete!\")\n",
        "    print(f\"   Saved to: {output_path}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "print(\" Main generation function loaded (FIXED)!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLbS-kdE1jO3",
        "outputId": "fecc414a-63af-43a4-94d0-6f889f16faf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "LOADING AIDA DATA\n",
            "======================================================================\n",
            "\n",
            " Loading from: data/processed/aida\n",
            "\n",
            "‚úì Train: 946 documents\n",
            "‚úì Validation: 216 documents\n",
            "‚úì Test: 231 documents\n",
            "\n",
            " Total entities:\n",
            "   Train: 23,393\n",
            "   Val: 5,916\n",
            "   Test: 5,614\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Cell 7: Load Data\n",
        "# ============================================================================\n",
        "\n",
        "df_train, df_val, df_test = load_aida_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD-C4DlM1nhJ",
        "outputId": "790745ee-f9e2-4444-fe95-30ccabc17ed2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Starting VALIDATION split generation...\n",
            "\n",
            "======================================================================\n",
            "PROCESSING: VAL\n",
            "======================================================================\n",
            "\n",
            "üîç Collecting unique mentions from val...\n",
            "   Unique mentions: 2597 (vs 5916 total)\n",
            "   Reduction: 56.1%\n",
            "\n",
            " Batched generation:\n",
            "   Batch size: 32\n",
            "   Total batches: 82\n",
            "   Estimated time: 0.7 minutes\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rGenerating batches:   0%|          | 0/82 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   1%|          | 1/82 [00:06<08:49,  6.54s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   2%|‚ñè         | 2/82 [00:13<08:47,  6.59s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   4%|‚ñé         | 3/82 [00:19<08:39,  6.57s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   5%|‚ñç         | 4/82 [00:24<07:45,  5.97s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   6%|‚ñå         | 5/82 [00:27<06:03,  4.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   7%|‚ñã         | 6/82 [00:30<05:07,  4.05s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   9%|‚ñä         | 7/82 [00:33<04:43,  3.78s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  10%|‚ñâ         | 8/82 [00:36<04:28,  3.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  11%|‚ñà         | 9/82 [00:39<04:03,  3.34s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  12%|‚ñà‚ñè        | 10/82 [00:41<03:43,  3.11s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  13%|‚ñà‚ñé        | 11/82 [00:44<03:29,  2.95s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  15%|‚ñà‚ñç        | 12/82 [00:48<03:43,  3.19s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  16%|‚ñà‚ñå        | 13/82 [00:51<03:40,  3.20s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  17%|‚ñà‚ñã        | 14/82 [00:53<03:22,  2.98s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  18%|‚ñà‚ñä        | 15/82 [00:55<02:51,  2.55s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  20%|‚ñà‚ñâ        | 16/82 [00:57<02:29,  2.27s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  21%|‚ñà‚ñà        | 17/82 [00:58<02:13,  2.06s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  22%|‚ñà‚ñà‚ñè       | 18/82 [01:00<02:07,  1.99s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  23%|‚ñà‚ñà‚ñé       | 19/82 [01:02<02:03,  1.96s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  24%|‚ñà‚ñà‚ñç       | 20/82 [01:03<01:54,  1.84s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  26%|‚ñà‚ñà‚ñå       | 21/82 [01:05<01:47,  1.77s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  27%|‚ñà‚ñà‚ñã       | 22/82 [01:07<01:42,  1.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  28%|‚ñà‚ñà‚ñä       | 23/82 [01:08<01:39,  1.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  29%|‚ñà‚ñà‚ñâ       | 24/82 [01:10<01:35,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  30%|‚ñà‚ñà‚ñà       | 25/82 [01:12<01:36,  1.70s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  32%|‚ñà‚ñà‚ñà‚ñè      | 26/82 [01:14<01:39,  1.77s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  33%|‚ñà‚ñà‚ñà‚ñé      | 27/82 [01:15<01:39,  1.82s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  34%|‚ñà‚ñà‚ñà‚ñç      | 28/82 [01:17<01:35,  1.76s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  35%|‚ñà‚ñà‚ñà‚ñå      | 29/82 [01:19<01:31,  1.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  37%|‚ñà‚ñà‚ñà‚ñã      | 30/82 [01:20<01:30,  1.74s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  38%|‚ñà‚ñà‚ñà‚ñä      | 31/82 [01:22<01:31,  1.80s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  39%|‚ñà‚ñà‚ñà‚ñâ      | 32/82 [01:24<01:29,  1.80s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  40%|‚ñà‚ñà‚ñà‚ñà      | 33/82 [01:26<01:29,  1.82s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 34/82 [01:28<01:24,  1.75s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 35/82 [01:29<01:20,  1.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 36/82 [01:31<01:18,  1.70s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 37/82 [01:33<01:16,  1.69s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 38/82 [01:34<01:13,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 39/82 [01:36<01:14,  1.74s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 40/82 [01:40<01:37,  2.32s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 41/82 [01:42<01:29,  2.19s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 42/82 [01:43<01:20,  2.02s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 43/82 [01:45<01:13,  1.89s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 44/82 [01:47<01:08,  1.81s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 45/82 [01:48<01:06,  1.79s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 46/82 [01:50<01:05,  1.82s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 47/82 [01:52<01:01,  1.76s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 48/82 [01:53<00:58,  1.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 49/82 [01:55<00:54,  1.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 50/82 [01:57<00:52,  1.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 51/82 [02:03<01:36,  3.12s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 52/82 [02:05<01:21,  2.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 53/82 [02:06<01:08,  2.37s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 54/82 [02:08<00:59,  2.13s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 55/82 [02:10<00:52,  1.96s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 56/82 [02:11<00:48,  1.85s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 57/82 [02:13<00:44,  1.77s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 58/82 [02:15<00:43,  1.83s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 59/82 [02:16<00:40,  1.78s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 60/82 [02:18<00:37,  1.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 61/82 [02:20<00:35,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 62/82 [02:21<00:32,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 63/82 [02:23<00:31,  1.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 64/82 [02:24<00:29,  1.62s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 65/82 [02:26<00:28,  1.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 66/82 [02:28<00:27,  1.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 67/82 [02:30<00:25,  1.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 68/82 [02:31<00:23,  1.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 69/82 [02:33<00:21,  1.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 70/82 [02:35<00:20,  1.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 71/82 [02:36<00:18,  1.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 72/82 [02:38<00:17,  1.70s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 73/82 [02:40<00:15,  1.76s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 74/82 [02:41<00:13,  1.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 75/82 [02:43<00:11,  1.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 76/82 [02:45<00:09,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 77/82 [02:46<00:08,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 78/82 [02:48<00:06,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 79/82 [02:50<00:04,  1.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 80/82 [02:52<00:03,  1.74s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 81/82 [02:53<00:01,  1.69s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 82/82 [02:54<00:00,  2.13s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Mapping clarifications to documents...\n",
            "\n",
            " VAL complete!\n",
            "   Saved to: data/experiments/clarifications_val.json\n",
            "\n",
            " Validation complete: 216 documents processed\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Cell 8: Generate Clarifications - VALIDATION SPLIT\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n Starting VALIDATION split generation...\")\n",
        "\n",
        "val_clarifications = generate_clarifications_for_split(model, tokenizer, df_val, 'val')\n",
        "\n",
        "print(f\"\\n Validation complete: {len(val_clarifications)} documents processed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDSN7s-b1peb",
        "outputId": "988e8ac4-bcc4-4e15-a122-753dfff898d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Starting TEST split generation...\n",
            "\n",
            "======================================================================\n",
            "PROCESSING: TEST\n",
            "======================================================================\n",
            "\n",
            "üîç Collecting unique mentions from test...\n",
            "   Unique mentions: 2442 (vs 5614 total)\n",
            "   Reduction: 56.5%\n",
            "\n",
            " Batched generation:\n",
            "   Batch size: 32\n",
            "   Total batches: 77\n",
            "   Estimated time: 0.6 minutes\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating batches:   0%|          | 0/77 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   1%|‚ñè         | 1/77 [00:01<01:59,  1.57s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   3%|‚ñé         | 2/77 [00:03<01:58,  1.58s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   4%|‚ñç         | 3/77 [00:04<01:57,  1.58s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   5%|‚ñå         | 4/77 [00:06<01:57,  1.60s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   6%|‚ñã         | 5/77 [00:08<02:04,  1.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   8%|‚ñä         | 6/77 [00:09<02:00,  1.70s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   9%|‚ñâ         | 7/77 [00:11<01:56,  1.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  10%|‚ñà         | 8/77 [00:13<01:52,  1.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  12%|‚ñà‚ñè        | 9/77 [00:14<01:50,  1.62s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  13%|‚ñà‚ñé        | 10/77 [00:16<01:48,  1.61s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  14%|‚ñà‚ñç        | 11/77 [00:17<01:47,  1.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  16%|‚ñà‚ñå        | 12/77 [00:19<01:48,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  17%|‚ñà‚ñã        | 13/77 [00:21<01:50,  1.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  18%|‚ñà‚ñä        | 14/77 [00:23<01:47,  1.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  19%|‚ñà‚ñâ        | 15/77 [00:24<01:43,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  21%|‚ñà‚ñà        | 16/77 [00:26<01:40,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  22%|‚ñà‚ñà‚ñè       | 17/77 [00:28<01:38,  1.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  23%|‚ñà‚ñà‚ñé       | 18/77 [00:29<01:36,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  25%|‚ñà‚ñà‚ñç       | 19/77 [00:31<01:35,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  26%|‚ñà‚ñà‚ñå       | 20/77 [00:33<01:38,  1.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  27%|‚ñà‚ñà‚ñã       | 21/77 [00:34<01:34,  1.69s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  29%|‚ñà‚ñà‚ñä       | 22/77 [00:36<01:31,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  30%|‚ñà‚ñà‚ñâ       | 23/77 [00:38<01:28,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  31%|‚ñà‚ñà‚ñà       | 24/77 [00:39<01:26,  1.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  32%|‚ñà‚ñà‚ñà‚ñè      | 25/77 [00:41<01:24,  1.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  34%|‚ñà‚ñà‚ñà‚ñç      | 26/77 [00:42<01:22,  1.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  35%|‚ñà‚ñà‚ñà‚ñå      | 27/77 [00:44<01:26,  1.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  36%|‚ñà‚ñà‚ñà‚ñã      | 28/77 [00:46<01:23,  1.70s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  38%|‚ñà‚ñà‚ñà‚ñä      | 29/77 [00:48<01:20,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  39%|‚ñà‚ñà‚ñà‚ñâ      | 30/77 [00:49<01:18,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  40%|‚ñà‚ñà‚ñà‚ñà      | 31/77 [00:51<01:16,  1.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 32/77 [00:53<01:13,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 33/77 [00:54<01:11,  1.62s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 34/77 [00:56<01:12,  1.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 35/77 [00:58<01:12,  1.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 36/77 [00:59<01:08,  1.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 37/77 [01:01<01:06,  1.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 38/77 [01:03<01:05,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 39/77 [01:04<01:02,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 40/77 [01:06<01:00,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 41/77 [01:08<00:59,  1.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 42/77 [01:10<01:01,  1.77s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 43/77 [01:11<00:59,  1.74s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 44/77 [01:13<00:55,  1.70s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 45/77 [01:15<00:55,  1.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 46/77 [01:16<00:52,  1.70s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 47/77 [01:18<00:49,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 48/77 [01:20<00:48,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 49/77 [01:21<00:49,  1.75s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 50/77 [01:23<00:46,  1.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 51/77 [01:25<00:43,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 52/77 [01:26<00:41,  1.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 53/77 [01:28<00:39,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 54/77 [01:30<00:37,  1.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 55/77 [01:31<00:35,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 56/77 [01:33<00:36,  1.74s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 57/77 [01:35<00:34,  1.70s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 58/77 [01:36<00:32,  1.69s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 59/77 [01:38<00:29,  1.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 60/77 [01:40<00:28,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 61/77 [01:41<00:26,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 62/77 [01:43<00:24,  1.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 63/77 [01:45<00:23,  1.69s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 64/77 [01:47<00:22,  1.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 65/77 [01:48<00:20,  1.69s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 66/77 [01:50<00:18,  1.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 67/77 [01:51<00:16,  1.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 68/77 [01:53<00:14,  1.62s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 69/77 [01:54<00:12,  1.60s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 70/77 [01:56<00:11,  1.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 71/77 [01:58<00:10,  1.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 72/77 [02:00<00:08,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 73/77 [02:01<00:06,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 74/77 [02:03<00:04,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 75/77 [02:04<00:03,  1.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 76/77 [02:06<00:01,  1.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [02:07<00:00,  1.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Mapping clarifications to documents...\n",
            "\n",
            " TEST complete!\n",
            "   Saved to: data/experiments/clarifications_test.json\n",
            "\n",
            " Test complete: 231 documents processed\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Cell 9: Generate Clarifications - TEST SPLIT\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n Starting TEST split generation...\")\n",
        "\n",
        "test_clarifications = generate_clarifications_for_split(model, tokenizer, df_test, 'test')\n",
        "\n",
        "print(f\"\\n Test complete: {len(test_clarifications)} documents processed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgMp1Vim1ttB",
        "outputId": "c353ebfa-1f80-4b88-e1d9-52136bb39552"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Starting TRAIN split generation...\n",
            "\n",
            "======================================================================\n",
            "PROCESSING: TRAIN\n",
            "======================================================================\n",
            "\n",
            "üîç Collecting unique mentions from train...\n",
            "   Unique mentions: 7542 (vs 23393 total)\n",
            "   Reduction: 67.8%\n",
            "\n",
            " Batched generation:\n",
            "   Batch size: 32\n",
            "   Total batches: 236\n",
            "   Estimated time: 2.0 minutes\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating batches:   0%|          | 0/236 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   0%|          | 1/236 [00:01<07:31,  1.92s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   1%|          | 2/236 [00:03<06:48,  1.74s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   1%|‚ñè         | 3/236 [00:05<06:35,  1.70s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   2%|‚ñè         | 4/236 [00:06<06:24,  1.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   2%|‚ñè         | 5/236 [00:08<06:19,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   3%|‚ñé         | 6/236 [00:09<06:12,  1.62s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   3%|‚ñé         | 7/236 [00:11<06:08,  1.61s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   3%|‚ñé         | 8/236 [00:13<06:21,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   4%|‚ñç         | 9/236 [00:15<06:29,  1.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   4%|‚ñç         | 10/236 [00:16<06:17,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   5%|‚ñç         | 11/236 [00:18<06:11,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   5%|‚ñå         | 12/236 [00:19<06:04,  1.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   6%|‚ñå         | 13/236 [00:21<06:02,  1.62s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   6%|‚ñå         | 14/236 [00:23<05:57,  1.61s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   6%|‚ñã         | 15/236 [00:24<06:03,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   7%|‚ñã         | 16/236 [00:26<06:18,  1.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   7%|‚ñã         | 17/236 [00:28<06:13,  1.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   8%|‚ñä         | 18/236 [00:30<06:05,  1.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   8%|‚ñä         | 19/236 [00:31<05:57,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   8%|‚ñä         | 20/236 [00:33<05:54,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   9%|‚ñâ         | 21/236 [00:34<05:48,  1.62s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:   9%|‚ñâ         | 22/236 [00:36<05:46,  1.62s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  10%|‚ñâ         | 23/236 [00:38<06:06,  1.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  10%|‚ñà         | 24/236 [00:40<06:21,  1.80s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  11%|‚ñà         | 25/236 [00:42<06:30,  1.85s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  11%|‚ñà         | 26/236 [00:43<06:14,  1.78s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  11%|‚ñà‚ñè        | 27/236 [00:45<06:01,  1.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  12%|‚ñà‚ñè        | 28/236 [00:47<05:48,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  12%|‚ñà‚ñè        | 29/236 [00:48<05:45,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  13%|‚ñà‚ñé        | 30/236 [00:50<06:02,  1.76s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  13%|‚ñà‚ñé        | 31/236 [00:52<05:49,  1.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  14%|‚ñà‚ñé        | 32/236 [00:53<05:41,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  14%|‚ñà‚ñç        | 33/236 [00:55<05:33,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  14%|‚ñà‚ñç        | 34/236 [00:57<05:31,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  15%|‚ñà‚ñç        | 35/236 [00:58<05:26,  1.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  15%|‚ñà‚ñå        | 36/236 [01:00<05:23,  1.62s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  16%|‚ñà‚ñå        | 37/236 [01:02<05:34,  1.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  16%|‚ñà‚ñå        | 38/236 [01:03<05:38,  1.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  17%|‚ñà‚ñã        | 39/236 [01:05<05:29,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  17%|‚ñà‚ñã        | 40/236 [01:07<05:22,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  17%|‚ñà‚ñã        | 41/236 [01:08<05:17,  1.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  18%|‚ñà‚ñä        | 42/236 [01:10<05:12,  1.61s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  18%|‚ñà‚ñä        | 43/236 [01:11<05:10,  1.61s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  19%|‚ñà‚ñä        | 44/236 [01:13<05:14,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  19%|‚ñà‚ñâ        | 45/236 [01:15<05:27,  1.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  19%|‚ñà‚ñâ        | 46/236 [01:17<05:20,  1.69s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  20%|‚ñà‚ñâ        | 47/236 [01:18<05:12,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  20%|‚ñà‚ñà        | 48/236 [01:20<05:09,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  21%|‚ñà‚ñà        | 49/236 [01:21<05:05,  1.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  21%|‚ñà‚ñà        | 50/236 [01:23<05:02,  1.62s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  22%|‚ñà‚ñà‚ñè       | 51/236 [01:25<05:02,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  22%|‚ñà‚ñà‚ñè       | 52/236 [01:27<05:19,  1.74s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  22%|‚ñà‚ñà‚ñè       | 53/236 [01:28<05:11,  1.70s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  23%|‚ñà‚ñà‚ñé       | 54/236 [01:30<05:05,  1.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  23%|‚ñà‚ñà‚ñé       | 55/236 [01:31<04:59,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  24%|‚ñà‚ñà‚ñé       | 56/236 [01:33<04:54,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  24%|‚ñà‚ñà‚ñç       | 57/236 [01:35<04:51,  1.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  25%|‚ñà‚ñà‚ñç       | 58/236 [01:36<04:47,  1.61s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  25%|‚ñà‚ñà‚ñå       | 59/236 [01:38<05:00,  1.70s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  25%|‚ñà‚ñà‚ñå       | 60/236 [01:40<05:00,  1.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  26%|‚ñà‚ñà‚ñå       | 61/236 [01:41<04:53,  1.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  26%|‚ñà‚ñà‚ñã       | 62/236 [01:43<04:48,  1.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  27%|‚ñà‚ñà‚ñã       | 63/236 [01:45<04:43,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  27%|‚ñà‚ñà‚ñã       | 64/236 [01:46<04:40,  1.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  28%|‚ñà‚ñà‚ñä       | 65/236 [01:48<04:37,  1.62s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  28%|‚ñà‚ñà‚ñä       | 66/236 [01:50<04:42,  1.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  28%|‚ñà‚ñà‚ñä       | 67/236 [01:52<04:52,  1.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  29%|‚ñà‚ñà‚ñâ       | 68/236 [01:53<04:42,  1.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  29%|‚ñà‚ñà‚ñâ       | 69/236 [01:55<04:38,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  30%|‚ñà‚ñà‚ñâ       | 70/236 [01:56<04:35,  1.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  30%|‚ñà‚ñà‚ñà       | 71/236 [01:58<04:31,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  31%|‚ñà‚ñà‚ñà       | 72/236 [02:00<04:29,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  31%|‚ñà‚ñà‚ñà       | 73/236 [02:01<04:28,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  31%|‚ñà‚ñà‚ñà‚ñè      | 74/236 [02:03<04:42,  1.75s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  32%|‚ñà‚ñà‚ñà‚ñè      | 75/236 [02:05<04:35,  1.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  32%|‚ñà‚ñà‚ñà‚ñè      | 76/236 [02:07<04:28,  1.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  33%|‚ñà‚ñà‚ñà‚ñé      | 77/236 [02:08<04:25,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  33%|‚ñà‚ñà‚ñà‚ñé      | 78/236 [02:10<04:20,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  33%|‚ñà‚ñà‚ñà‚ñé      | 79/236 [02:11<04:17,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  34%|‚ñà‚ñà‚ñà‚ñç      | 80/236 [02:13<04:12,  1.62s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  34%|‚ñà‚ñà‚ñà‚ñç      | 81/236 [02:15<04:26,  1.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  35%|‚ñà‚ñà‚ñà‚ñç      | 82/236 [02:17<04:20,  1.69s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  35%|‚ñà‚ñà‚ñà‚ñå      | 83/236 [02:18<04:15,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  36%|‚ñà‚ñà‚ñà‚ñå      | 84/236 [02:20<04:10,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  36%|‚ñà‚ñà‚ñà‚ñå      | 85/236 [02:21<04:06,  1.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  36%|‚ñà‚ñà‚ñà‚ñã      | 86/236 [02:23<04:03,  1.62s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  37%|‚ñà‚ñà‚ñà‚ñã      | 87/236 [02:25<04:00,  1.61s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  37%|‚ñà‚ñà‚ñà‚ñã      | 88/236 [02:26<04:05,  1.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  38%|‚ñà‚ñà‚ñà‚ñä      | 89/236 [02:28<04:14,  1.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  38%|‚ñà‚ñà‚ñà‚ñä      | 90/236 [02:30<04:09,  1.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  39%|‚ñà‚ñà‚ñà‚ñä      | 91/236 [02:31<04:01,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  39%|‚ñà‚ñà‚ñà‚ñâ      | 92/236 [02:33<03:56,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  39%|‚ñà‚ñà‚ñà‚ñâ      | 93/236 [02:35<03:53,  1.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  40%|‚ñà‚ñà‚ñà‚ñâ      | 94/236 [02:36<03:50,  1.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  40%|‚ñà‚ñà‚ñà‚ñà      | 95/236 [02:38<03:51,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  41%|‚ñà‚ñà‚ñà‚ñà      | 96/236 [02:40<04:01,  1.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  41%|‚ñà‚ñà‚ñà‚ñà      | 97/236 [02:41<03:55,  1.69s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 98/236 [02:43<03:49,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 99/236 [02:45<03:45,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 100/236 [02:46<03:41,  1.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 101/236 [02:48<03:39,  1.62s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 102/236 [02:49<03:37,  1.62s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 103/236 [02:51<03:48,  1.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 104/236 [02:53<03:42,  1.69s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 105/236 [02:55<03:37,  1.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 106/236 [02:56<03:34,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 107/236 [02:58<03:30,  1.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 108/236 [02:59<03:29,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 109/236 [03:01<03:28,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 110/236 [03:03<03:34,  1.70s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 111/236 [03:05<03:35,  1.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 112/236 [03:06<03:28,  1.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 113/236 [03:08<03:23,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 114/236 [03:10<03:19,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 115/236 [03:11<03:16,  1.62s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 116/236 [03:13<03:13,  1.62s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 117/236 [03:14<03:15,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 118/236 [03:16<03:23,  1.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 119/236 [03:18<03:17,  1.69s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 120/236 [03:20<03:13,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 121/236 [03:21<03:10,  1.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 122/236 [03:23<03:06,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 123/236 [03:24<03:06,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 124/236 [03:26<03:04,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 125/236 [03:28<03:13,  1.74s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 126/236 [03:30<03:07,  1.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 127/236 [03:31<03:05,  1.70s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 128/236 [03:33<03:01,  1.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 129/236 [03:35<02:57,  1.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 130/236 [03:36<02:54,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 131/236 [03:38<02:51,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 132/236 [03:40<02:59,  1.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 133/236 [03:42<02:57,  1.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 134/236 [03:43<02:51,  1.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 135/236 [03:45<02:48,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 136/236 [03:46<02:43,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 137/236 [03:48<02:41,  1.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 138/236 [03:50<02:38,  1.62s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 139/236 [03:51<02:41,  1.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 140/236 [03:53<02:45,  1.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 141/236 [03:55<02:40,  1.69s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 142/236 [03:56<02:36,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 143/236 [03:58<02:33,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 144/236 [04:00<02:30,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 145/236 [04:01<02:29,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 146/236 [04:03<02:28,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 147/236 [04:05<02:34,  1.74s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 148/236 [04:07<02:31,  1.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 149/236 [04:08<02:26,  1.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 150/236 [04:10<02:23,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 151/236 [04:11<02:20,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 152/236 [04:13<02:17,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 153/236 [04:15<02:16,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 154/236 [04:17<02:22,  1.74s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 155/236 [04:18<02:18,  1.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 156/236 [04:20<02:14,  1.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 157/236 [04:21<02:10,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 158/236 [04:23<02:07,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 159/236 [04:25<02:05,  1.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 160/236 [04:26<02:04,  1.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 161/236 [04:28<02:08,  1.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 162/236 [04:30<02:08,  1.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 163/236 [04:32<02:04,  1.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 164/236 [04:33<02:00,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 165/236 [04:35<01:57,  1.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 166/236 [04:36<01:54,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 167/236 [04:38<01:52,  1.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 168/236 [04:40<01:53,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 169/236 [04:42<01:57,  1.75s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 170/236 [04:43<01:52,  1.70s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 171/236 [04:45<01:49,  1.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 172/236 [04:47<01:46,  1.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 173/236 [04:48<01:43,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 174/236 [04:50<01:41,  1.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 175/236 [04:52<01:41,  1.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 176/236 [04:53<01:44,  1.74s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 177/236 [04:55<01:40,  1.70s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 178/236 [04:57<01:36,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 179/236 [04:58<01:33,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 180/236 [05:00<01:32,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 181/236 [05:02<01:30,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 182/236 [05:03<01:27,  1.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 183/236 [05:06<01:41,  1.92s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 184/236 [05:08<01:41,  1.94s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 185/236 [05:09<01:33,  1.84s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 186/236 [05:11<01:28,  1.77s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 187/236 [05:13<01:24,  1.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 188/236 [05:14<01:21,  1.69s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 189/236 [05:16<01:18,  1.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 190/236 [05:18<01:17,  1.70s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 191/236 [05:20<01:20,  1.78s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 192/236 [05:21<01:16,  1.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 193/236 [05:23<01:13,  1.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 194/236 [05:24<01:11,  1.69s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 195/236 [05:26<01:09,  1.70s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 196/236 [05:28<01:06,  1.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 197/236 [05:30<01:05,  1.69s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 198/236 [05:31<01:06,  1.76s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 199/236 [05:33<01:03,  1.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 200/236 [05:35<01:01,  1.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 201/236 [05:36<00:59,  1.70s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 202/236 [05:38<00:57,  1.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 203/236 [05:40<00:55,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 204/236 [05:41<00:53,  1.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 205/236 [05:43<00:54,  1.77s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 206/236 [05:45<00:51,  1.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 207/236 [05:47<00:49,  1.70s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 208/236 [05:48<00:46,  1.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 209/236 [05:50<00:44,  1.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 210/236 [05:52<00:42,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 211/236 [05:53<00:40,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 212/236 [05:55<00:41,  1.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 213/236 [05:57<00:39,  1.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 214/236 [05:58<00:37,  1.69s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 215/236 [06:00<00:35,  1.69s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 216/236 [06:02<00:33,  1.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 217/236 [06:03<00:31,  1.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 218/236 [06:05<00:29,  1.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 219/236 [06:07<00:28,  1.69s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 220/236 [06:09<00:27,  1.74s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 221/236 [06:10<00:25,  1.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 222/236 [06:12<00:23,  1.70s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 223/236 [06:14<00:21,  1.69s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 224/236 [06:15<00:20,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 225/236 [06:17<00:18,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 226/236 [06:19<00:17,  1.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 227/236 [06:21<00:15,  1.77s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 228/236 [06:22<00:13,  1.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 229/236 [06:24<00:11,  1.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 230/236 [06:25<00:10,  1.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 231/236 [06:27<00:08,  1.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 232/236 [06:29<00:06,  1.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 233/236 [06:30<00:05,  1.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 234/236 [06:32<00:03,  1.77s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 235/236 [06:34<00:01,  1.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 236/236 [06:36<00:00,  1.68s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Mapping clarifications to documents...\n",
            "\n",
            " TRAIN complete!\n",
            "   Saved to: data/experiments/clarifications_train.json\n",
            "\n",
            " Train complete: 946 documents processed\n",
            "\n",
            " GPU memory cleared\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Cell 10: Generate Clarifications - TRAIN SPLIT\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n Starting TRAIN split generation...\")\n",
        "\n",
        "train_clarifications = generate_clarifications_for_split(model, tokenizer, df_train, 'train')\n",
        "\n",
        "print(f\"\\n Train complete: {len(train_clarifications)} documents processed\")\n",
        "\n",
        "# Clear GPU cache\n",
        "torch.cuda.empty_cache()\n",
        "print(\"\\n GPU memory cleared\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "RvCGzyUf5G5B",
        "outputId": "e78fb2b7-1c92-4744-f0ec-724bff220401"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Preparing download package...\n",
            "\n",
            " Downloading results...\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_f59a1a94-070c-45c6-aee9-6ba33e88435a\", \"clarifications_results.zip\", 10464006)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Download complete!\n",
            "\n",
            "Files included:\n",
            "   - clarifications_val.json\n",
            "   - clarifications_test.json\n",
            "   - clarifications_train.json\n",
            "   - checkpoints/ (backup files)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Cell 11: Download Results\n",
        "# ============================================================================\n",
        "\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "print(\" Preparing download package...\")\n",
        "\n",
        "# Create zip file with all results\n",
        "shutil.make_archive('clarifications_results', 'zip', CONFIG['output_dir'])\n",
        "\n",
        "print(\"\\n Downloading results...\")\n",
        "files.download('clarifications_results.zip')\n",
        "\n",
        "print(\"\\n Download complete!\")\n",
        "print(f\"\\nFiles included:\")\n",
        "print(f\"   - clarifications_val.json\")\n",
        "print(f\"   - clarifications_test.json\")\n",
        "print(f\"   - clarifications_train.json\")\n",
        "print(f\"   - checkpoints/ (backup files)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMHz5aoN5K6h",
        "outputId": "8f63e7d1-70a9-4807-d613-e32f40dbf0f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Sample Results Preview\n",
            "\n",
            "\n",
            " Document ID: 0\n",
            "\n",
            " Text (first 200 chars):\n",
            "CRICKET - LEICESTERSHIRE TAKE OVER AT TOP AFTER INNINGS VICTORY . LONDON 1996-08-30 West Indian all-rounder Phil Simmons took four for 38 on Friday as Leicestershire beat Somerset by an innings and 39...\n",
            "\n",
            "  Entities and Clarifications:\n",
            "\n",
            "   ‚Ä¢ LEICESTERSHIRE\n",
            "     ‚Üí \"LEICESTERSHIRE\" is a county in the East Midlands region of England. It is bordered by Lincolnshire to the north, Rutland to the east, Northamptonshire to the south-east, and Derbyshire to the south-west\n",
            "\n",
            "   ‚Ä¢ LONDON\n",
            "     ‚Üí #LONDON# is a city in the United Kingdom. It is the capital of England and the United Kingdom. It is the largest city in the United Kingdom and the United Kingdom's most populous city. It is the most populous city in the\n",
            "\n",
            "   ‚Ä¢ West Indian\n",
            "     ‚Üí Question: What is the name of the West Indian?\n",
            "Explanation: The West Indian is a region of the Caribbean Sea, which is located between the Caribbean Sea and the Atlantic Ocean. It is bordered by the Caribbean Sea to the north, the Gulf\n",
            "\n",
            " Statistics:\n",
            "   Total entities: 49\n",
            "   Total clarifications: 33\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Cell 12: View Sample Results\n",
        "# ============================================================================\n",
        "\n",
        "print(\" Sample Results Preview\\n\")\n",
        "\n",
        "# Show sample from validation set\n",
        "sample_doc = val_clarifications[0]\n",
        "\n",
        "print(f\"\\n Document ID: {sample_doc['doc_id']}\")\n",
        "print(f\"\\n Text (first 200 chars):\")\n",
        "print(sample_doc['text'][:200] + \"...\")\n",
        "\n",
        "print(f\"\\n  Entities and Clarifications:\")\n",
        "for entity in sample_doc['entities'][:3]:  # Show first 3\n",
        "    mention = entity['mention']\n",
        "    clarification = sample_doc['clarifications'][mention]\n",
        "    print(f\"\\n   ‚Ä¢ {mention}\")\n",
        "    print(f\"     ‚Üí {clarification}\")\n",
        "\n",
        "print(f\"\\n Statistics:\")\n",
        "print(f\"   Total entities: {sample_doc.get('num_entities', len(sample_doc['entities']))}\")\n",
        "print(f\"   Total clarifications: {len(sample_doc['clarifications'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uz9Z7s8d6LOY",
        "outputId": "437d68a1-6e6e-4ab4-8bce-f4ade699131d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "CREATING TRAINING DATASETS (FIXED)\n",
            "======================================================================\n",
            "\n",
            "üîß Processing train split for training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating train samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 946/946 [00:00<00:00, 11561.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Created 18541 baseline samples\n",
            "‚úì Created 18541 clarified samples\n",
            "\n",
            "üîß Processing val split for training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating val samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 216/216 [00:00<00:00, 9331.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Created 4791 baseline samples\n",
            "‚úì Created 4791 clarified samples\n",
            "\n",
            "üîß Processing test split for training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating test samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231/231 [00:00<00:00, 10398.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Created 4483 baseline samples\n",
            "‚úì Created 4483 clarified samples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Training datasets saved!\n",
            "\n",
            "üìã Sample Preview:\n",
            "\n",
            "1Ô∏è‚É£ Baseline sample:\n",
            "   Input: link entity: EU rejects [START_ENT] German [END_ENT] call to boycott British lamb . Peter Blackburn BRUSSELS 1996-08-22 The European Commission said o...\n",
            "   Target: Q183\n",
            "\n",
            "2Ô∏è‚É£ Clarified sample:\n",
            "   Input: link entity: EU rejects [START_ENT] German [END_ENT] [CLARIFY: # German is a language spoken by 100 million people in Germany and 100 million people i...\n",
            "   Target: Q183\n",
            "\n",
            "üìä Dataset Statistics:\n",
            "   Train baseline: 18541 samples\n",
            "   Train clarified: 18541 samples\n",
            "   Val baseline: 4791 samples\n",
            "   Val clarified: 4791 samples\n",
            "   Test baseline: 4483 samples\n",
            "   Test clarified: 4483 samples\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Cell 13: Create Augmented Datasets (FIXED WITH TASK PREFIX)\n",
        "# ============================================================================\n",
        "\n",
        "def create_training_samples(doc, use_clarifications=False):\n",
        "    \"\"\"\n",
        "    Create individual training samples for each entity in the document.\n",
        "\n",
        "    Key additions:\n",
        "    - Task prefix: \"link entity:\"\n",
        "    - Q prefix for Wikidata format\n",
        "    - Better context windowing\n",
        "    \"\"\"\n",
        "    text = doc['text']\n",
        "    entities = doc['entities']\n",
        "    clarifications = doc.get('clarifications', {})\n",
        "\n",
        "    samples = []\n",
        "\n",
        "    for entity in entities:\n",
        "        mention = entity.get('mention', '')\n",
        "        qid = entity.get('qid', 'NIL')\n",
        "        start = entity.get('start', 0)\n",
        "        end = entity.get('end', 0)\n",
        "\n",
        "        # Skip if no valid QID\n",
        "        if qid == 'NIL' or qid is None:\n",
        "            continue\n",
        "\n",
        "        # Clean QID (remove .0 decimal if present)\n",
        "        qid_clean = str(qid).replace('.0', '')\n",
        "\n",
        "        # Get context around entity (better than full text)\n",
        "        context_window = 250\n",
        "        context_left = text[max(0, start - context_window):start]\n",
        "        context_right = text[end:min(len(text), end + context_window)]\n",
        "\n",
        "        # Build marked entity\n",
        "        if use_clarifications and mention in clarifications:\n",
        "            clarification = clarifications[mention]\n",
        "            marked_entity = f\"[START_ENT] {mention} [END_ENT] [CLARIFY: {clarification}]\"\n",
        "        else:\n",
        "            marked_entity = f\"[START_ENT] {mention} [END_ENT]\"\n",
        "\n",
        "        # ‚úÖ ADD TASK PREFIX: This tells T5 what to do\n",
        "        input_text = f\"link entity: {context_left}{marked_entity}{context_right}\"\n",
        "\n",
        "        # Truncate if too long\n",
        "        if len(input_text) > 512:\n",
        "            input_text = input_text[:512]\n",
        "\n",
        "        # ‚úÖ TARGET FORMAT: Q + QID (standard Wikidata format)\n",
        "        target_text = f\"Q{qid_clean}\"\n",
        "\n",
        "        samples.append({\n",
        "            'input_text': input_text,\n",
        "            'target_text': target_text\n",
        "        })\n",
        "\n",
        "    return samples\n",
        "\n",
        "\n",
        "def process_split_for_training(clarifications_data, split_name):\n",
        "    \"\"\"\n",
        "    Convert clarification data to training format.\n",
        "\n",
        "    Creates TWO datasets:\n",
        "    1. Baseline: link entity: [START_ENT]mention[END_ENT] ‚Üí Q12345\n",
        "    2. Clarified: link entity: [START_ENT]mention[END_ENT][CLARIFY:...] ‚Üí Q12345\n",
        "    \"\"\"\n",
        "    print(f\"\\nüîß Processing {split_name} split for training...\")\n",
        "\n",
        "    baseline_samples = []\n",
        "    clarified_samples = []\n",
        "\n",
        "    for doc in tqdm(clarifications_data, desc=f\"Creating {split_name} samples\"):\n",
        "        # Create baseline samples (without clarifications)\n",
        "        baseline_samples.extend(create_training_samples(doc, use_clarifications=False))\n",
        "\n",
        "        # Create clarified samples (with clarifications)\n",
        "        clarified_samples.extend(create_training_samples(doc, use_clarifications=True))\n",
        "\n",
        "    print(f\"‚úì Created {len(baseline_samples)} baseline samples\")\n",
        "    print(f\"‚úì Created {len(clarified_samples)} clarified samples\")\n",
        "\n",
        "    return baseline_samples, clarified_samples\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CREATING TRAINING DATASETS (FIXED)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Process all splits\n",
        "train_baseline, train_clarified = process_split_for_training(train_clarifications, 'train')\n",
        "val_baseline, val_clarified = process_split_for_training(val_clarifications, 'val')\n",
        "test_baseline, test_clarified = process_split_for_training(test_clarifications, 'test')\n",
        "\n",
        "# Save processed datasets\n",
        "os.makedirs('data/experiments/processed_for_training', exist_ok=True)\n",
        "\n",
        "def save_samples(samples, filename):\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        for sample in samples:\n",
        "            f.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
        "\n",
        "save_samples(train_baseline, 'data/experiments/processed_for_training/train_baseline.jsonl')\n",
        "save_samples(train_clarified, 'data/experiments/processed_for_training/train_clarified.jsonl')\n",
        "save_samples(val_baseline, 'data/experiments/processed_for_training/val_baseline.jsonl')\n",
        "save_samples(val_clarified, 'data/experiments/processed_for_training/val_clarified.jsonl')\n",
        "save_samples(test_baseline, 'data/experiments/processed_for_training/test_baseline.jsonl')\n",
        "save_samples(test_clarified, 'data/experiments/processed_for_training/test_clarified.jsonl')\n",
        "\n",
        "print(\"\\n‚úÖ Training datasets saved!\")\n",
        "\n",
        "# Preview samples\n",
        "print(\"\\nüìã Sample Preview:\")\n",
        "print(\"\\n1Ô∏è‚É£ Baseline sample:\")\n",
        "print(f\"   Input: {train_baseline[0]['input_text'][:150]}...\")\n",
        "print(f\"   Target: {train_baseline[0]['target_text']}\")\n",
        "\n",
        "print(\"\\n2Ô∏è‚É£ Clarified sample:\")\n",
        "print(f\"   Input: {train_clarified[0]['input_text'][:150]}...\")\n",
        "print(f\"   Target: {train_clarified[0]['target_text']}\")\n",
        "\n",
        "print(\"\\nüìä Dataset Statistics:\")\n",
        "print(f\"   Train baseline: {len(train_baseline)} samples\")\n",
        "print(f\"   Train clarified: {len(train_clarified)} samples\")\n",
        "print(f\"   Val baseline: {len(val_baseline)} samples\")\n",
        "print(f\"   Val clarified: {len(val_clarified)} samples\")\n",
        "print(f\"   Test baseline: {len(test_baseline)} samples\")\n",
        "print(f\"   Test clarified: {len(test_clarified)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CMshGmVXG_J",
        "outputId": "762cde41-4cab-4157-964b-a0f8cb373292"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " FULL DATASET MODE\n",
            "   Train: 18541 samples\n",
            "   Val: 4791 samples\n",
            "   Test: 4483 samples\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Cell 13.5: QUICK TEST MODE - Use Small Subset\n",
        "# ============================================================================\n",
        "\n",
        "# ‚úÖ ENABLE THIS FOR FAST TESTING\n",
        "QUICK_TEST_MODE = False  # Set to False for full training\n",
        "\n",
        "if QUICK_TEST_MODE:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"‚ö° QUICK TEST MODE ENABLED\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\nUsing small subsets for fast validation:\")\n",
        "\n",
        "    # Use only first 50 samples from each split\n",
        "    test_size = 50\n",
        "\n",
        "    train_baseline = train_baseline[:test_size]\n",
        "    train_clarified = train_clarified[:test_size]\n",
        "    val_baseline = val_baseline[:test_size]\n",
        "    val_clarified = val_clarified[:test_size]\n",
        "    test_baseline = test_baseline[:test_size]\n",
        "    test_clarified = test_clarified[:test_size]\n",
        "\n",
        "    print(f\"   Train samples: {len(train_baseline)}\")\n",
        "    print(f\"   Val samples: {len(val_baseline)}\")\n",
        "    print(f\"   Test samples: {len(test_baseline)}\")\n",
        "    print(f\"\\n   Estimated time: ~5-10 minutes total\")\n",
        "    print(f\"   (vs ~2 hours for full dataset)\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n FULL DATASET MODE\")\n",
        "    print(f\"   Train: {len(train_baseline)} samples\")\n",
        "    print(f\"   Val: {len(val_baseline)} samples\")\n",
        "    print(f\"   Test: {len(test_baseline)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RUN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hX5-xyUt6v9e",
        "outputId": "3cf89de4-d23a-4349-f8fe-8a2dfb258e29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Preparing T5 model and tokenizer...\n",
            "‚úì T5 Tokenizer ready. Vocabulary size: 32103\n",
            "‚úì Train baseline: 18541 samples\n",
            "‚úì Train clarified: 18541 samples\n",
            "‚úì Val baseline: 4791 samples\n",
            "‚úì Val clarified: 4791 samples\n",
            "\n",
            " Datasets ready for training!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Cell 14: Prepare T5 Training (FIXED)\n",
        "# ============================================================================\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "\n",
        "class EntityLinkingDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for entity linking with T5.\"\"\"\n",
        "    def __init__(self, samples, tokenizer, max_length=512):\n",
        "        self.samples = samples\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            sample['input_text'],\n",
        "            text_target=sample['target_text'],\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {key: val.squeeze() for key, val in encoding.items()}\n",
        "\n",
        "\n",
        "def load_samples(filename):\n",
        "    \"\"\"Load JSONL samples.\"\"\"\n",
        "    samples = []\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            samples.append(json.loads(line))\n",
        "    return samples\n",
        "\n",
        "\n",
        "print(\"\\n Preparing T5 model and tokenizer...\")\n",
        "\n",
        "# Initialize T5 tokenizer (NEW - separate from clarification tokenizer)\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "\n",
        "# Add special tokens\n",
        "special_tokens = {\n",
        "    'additional_special_tokens': [\n",
        "        '[START_ENT]',\n",
        "        '[END_ENT]',\n",
        "        '[CLARIFY:',\n",
        "        ']'\n",
        "    ]\n",
        "}\n",
        "t5_tokenizer.add_special_tokens(special_tokens)\n",
        "\n",
        "print(f\"‚úì T5 Tokenizer ready. Vocabulary size: {len(t5_tokenizer)}\")\n",
        "\n",
        "\n",
        "train_baseline = load_samples('./data/processed/aida/clarifications_results/processed_for_training/train_baseline.jsonl')\n",
        "train_clarified = load_samples('./data/processed/aida/clarifications_results/processed_for_training/train_clarified.jsonl')\n",
        "val_baseline = load_samples('./data/processed/aida/clarifications_results/processed_for_training/val_baseline.jsonl')\n",
        "val_clarified = load_samples('./data/processed/aida/clarifications_results/processed_for_training/val_clarified.jsonl')\n",
        "\n",
        "# Create datasets using T5 tokenizer\n",
        "train_baseline_dataset = EntityLinkingDataset(train_baseline, t5_tokenizer)\n",
        "train_clarified_dataset = EntityLinkingDataset(train_clarified, t5_tokenizer)\n",
        "val_baseline_dataset = EntityLinkingDataset(val_baseline, t5_tokenizer)\n",
        "val_clarified_dataset = EntityLinkingDataset(val_clarified, t5_tokenizer)\n",
        "\n",
        "print(f\"‚úì Train baseline: {len(train_baseline_dataset)} samples\")\n",
        "print(f\"‚úì Train clarified: {len(train_clarified_dataset)} samples\")\n",
        "print(f\"‚úì Val baseline: {len(val_baseline_dataset)} samples\")\n",
        "print(f\"‚úì Val clarified: {len(val_clarified_dataset)} samples\")\n",
        "\n",
        "print(\"\\n Datasets ready for training!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HSnfc7x72lj",
        "outputId": "c108fd68-9aa1-40b8-a770-319a92c80f8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Clearing GPU memory...\n",
            "‚úì Clarification model deleted\n",
            "\n",
            " Ready for training!\n"
          ]
        }
      ],
      "source": [
        "# # ============================================================================\n",
        "# # Cell 14.5: Clear GPU Memory Before Training (FIXED)\n",
        "# # ============================================================================\n",
        "\n",
        "# import gc\n",
        "\n",
        "# print(\" Clearing GPU memory...\")\n",
        "\n",
        "# # Delete ONLY the clarification generation model (NOT t5_tokenizer)\n",
        "# if 'model' in globals():\n",
        "#     del model\n",
        "#     print(\"‚úì Clarification model deleted\")\n",
        "\n",
        "# # Note: We keep t5_tokenizer - it's needed for training!\n",
        "\n",
        "# # Force garbage collection\n",
        "# gc.collect()\n",
        "\n",
        "# # Clear PyTorch cache\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# # Check memory\n",
        "# if torch.cuda.is_available():\n",
        "#     print(f\"\\n‚úì GPU Memory freed\")\n",
        "#     print(f\"  Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "#     print(f\"  Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
        "#     print(f\"  Free: {torch.cuda.get_device_properties(0).total_memory / 1e9 - torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "\n",
        "# print(\"\\n Ready for training!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RUN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cwp67y5_WysS",
        "outputId": "fc8a1555-543a-442e-ec13-a5231f245fd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "CREATING SMALL SUBSET FOR QUICK EXPERIMENT\n",
            "======================================================================\n",
            "\n",
            "üìä Original dataset sizes:\n",
            "   Train baseline: 18541 samples\n",
            "   Train clarified: 18541 samples\n",
            "   Val baseline: 4791 samples\n",
            "   Val clarified: 4791 samples\n",
            "\n",
            "‚úÇÔ∏è  Subset sizes (10% of original):\n",
            "   Train baseline: 1854 samples\n",
            "   Train clarified: 1854 samples\n",
            "   Val baseline: 479 samples\n",
            "   Val clarified: 479 samples\n",
            "\n",
            "‚è±Ô∏è  Estimated training time:\n",
            "   Steps per epoch: 115\n",
            "   Total steps: 345\n",
            "   Estimated time per model: 2.9 minutes\n",
            "   Total time (both models): 5.8 minutes\n",
            "\n",
            "‚úÖ Small datasets ready for quick training!\n",
            "üí° This is perfect for experimentation and testing!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Cell 14.6: Create Small Subset for Quick Experiment (10% of data)\n",
        "# ============================================================================\n",
        "\n",
        "import random\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CREATING SMALL SUBSET FOR QUICK EXPERIMENT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Set seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Calculate subset sizes (10% of original)\n",
        "subset_percentage = 0.10\n",
        "\n",
        "print(f\"\\nüìä Original dataset sizes:\")\n",
        "print(f\"   Train baseline: {len(train_baseline)} samples\")\n",
        "print(f\"   Train clarified: {len(train_clarified)} samples\")\n",
        "print(f\"   Val baseline: {len(val_baseline)} samples\")\n",
        "print(f\"   Val clarified: {len(val_clarified)} samples\")\n",
        "\n",
        "# Create small subsets (10%)\n",
        "train_baseline_small = random.sample(train_baseline, int(len(train_baseline) * subset_percentage))\n",
        "train_clarified_small = random.sample(train_clarified, int(len(train_clarified) * subset_percentage))\n",
        "val_baseline_small = random.sample(val_baseline, int(len(val_baseline) * subset_percentage))\n",
        "val_clarified_small = random.sample(val_clarified, int(len(val_clarified) * subset_percentage))\n",
        "\n",
        "print(f\"\\n‚úÇÔ∏è  Subset sizes ({subset_percentage*100:.0f}% of original):\")\n",
        "print(f\"   Train baseline: {len(train_baseline_small)} samples\")\n",
        "print(f\"   Train clarified: {len(train_clarified_small)} samples\")\n",
        "print(f\"   Val baseline: {len(val_baseline_small)} samples\")\n",
        "print(f\"   Val clarified: {len(val_clarified_small)} samples\")\n",
        "\n",
        "# Create PyTorch datasets from subsets\n",
        "train_baseline_dataset = EntityLinkingDataset(train_baseline_small, t5_tokenizer)\n",
        "train_clarified_dataset = EntityLinkingDataset(train_clarified_small, t5_tokenizer)\n",
        "val_baseline_dataset = EntityLinkingDataset(val_baseline_small, t5_tokenizer)\n",
        "val_clarified_dataset = EntityLinkingDataset(val_clarified_small, t5_tokenizer)\n",
        "\n",
        "# Estimate new training time\n",
        "samples = len(train_baseline_dataset)\n",
        "batch_size = 8\n",
        "gradient_accumulation = 2\n",
        "effective_batch = batch_size * gradient_accumulation\n",
        "epochs = 3  # Reduced epochs for quick experiment\n",
        "\n",
        "steps_per_epoch = samples // effective_batch\n",
        "total_steps = steps_per_epoch * epochs\n",
        "estimated_minutes = total_steps * 0.5 / 60  # CPU: 0.5s per step\n",
        "\n",
        "print(f\"\\n‚è±Ô∏è  Estimated training time:\")\n",
        "print(f\"   Steps per epoch: {steps_per_epoch}\")\n",
        "print(f\"   Total steps: {total_steps}\")\n",
        "print(f\"   Estimated time per model: {estimated_minutes:.1f} minutes\")\n",
        "print(f\"   Total time (both models): {estimated_minutes * 2:.1f} minutes\")\n",
        "\n",
        "print(f\"\\n‚úÖ Small datasets ready for quick training!\")\n",
        "print(f\"üí° This is perfect for experimentation and testing!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RUN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvkSYdBYW2lg",
        "outputId": "035f0789-b49f-49e4-d8b3-33d52bb4d33f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üéØ (1) TRAINING BASELINE MODEL ON SMALL DATASET\n",
            "\n",
            "======================================================================\n",
            "TRAINING: BASELINE MODEL (QUICK EXPERIMENT)\n",
            "======================================================================\n",
            "\n",
            "üì¶ Loading T5-base...\n",
            "\n",
            "‚öôÔ∏è  Quick Training Config:\n",
            "   Dataset size: 1,854 samples (10% of full data)\n",
            "   Batch size: 8 (effective: 16)\n",
            "   Epochs: 3\n",
            "   Total steps: 345\n",
            "   ‚è±Ô∏è  Estimated time: ~2.9 minutes\n",
            "\n",
            "üöÄ Starting training...\n",
            "\n",
            "======================================================================\n",
            "üöÄ QUICK TRAINING STARTED (Small Dataset)\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Cell 15: Train Baseline Model (FAST - SMALL DATASET)\n",
        "# ============================================================================\n",
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    TrainerCallback,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "import time\n",
        "import json\n",
        "\n",
        "# ============================================================================\n",
        "# PROGRESS CALLBACK\n",
        "# ============================================================================\n",
        "\n",
        "class QuickProgressCallback(TrainerCallback):\n",
        "    \"\"\"Lightweight progress display for quick experiments.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.start_time = None\n",
        "        self.best_loss = float('inf')\n",
        "\n",
        "    def on_train_begin(self, args, state, control, **kwargs):\n",
        "        self.start_time = time.time()\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"üöÄ QUICK TRAINING STARTED (Small Dataset)\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "    def on_epoch_end(self, args, state, control, **kwargs):\n",
        "        epoch = int(state.epoch)\n",
        "\n",
        "        # Get eval loss\n",
        "        eval_loss = None\n",
        "        for log in reversed(state.log_history):\n",
        "            if 'eval_loss' in log:\n",
        "                eval_loss = log['eval_loss']\n",
        "                break\n",
        "\n",
        "        if eval_loss:\n",
        "            improvement = \"üìà NEW BEST!\" if eval_loss < self.best_loss else \"\"\n",
        "            self.best_loss = min(self.best_loss, eval_loss)\n",
        "            elapsed = time.time() - self.start_time\n",
        "            print(f\"   Epoch {epoch}: Val Loss = {eval_loss:.4f} | Time: {elapsed/60:.1f}min {improvement}\")\n",
        "\n",
        "    def on_train_end(self, args, state, control, **kwargs):\n",
        "        total_time = time.time() - self.start_time\n",
        "        print(f\"\\n‚úÖ Training completed in {total_time/60:.1f} minutes\")\n",
        "        print(f\"   Best validation loss: {self.best_loss:.4f}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# FAST TRAINING FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "def train_baseline_quick(train_dataset, val_dataset, output_dir='models/t5_baseline'):\n",
        "    \"\"\"Ultra-fast training for quick experiments.\"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"TRAINING: BASELINE MODEL (QUICK EXPERIMENT)\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Load model\n",
        "    print(\"\\nüì¶ Loading T5-base...\")\n",
        "    model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "    model.resize_token_embeddings(len(t5_tokenizer))\n",
        "\n",
        "    # Training config\n",
        "    total_samples = len(train_dataset)\n",
        "    batch_size = 8\n",
        "    gradient_accumulation = 2\n",
        "    num_epochs = 3  # ‚ö° Only 3 epochs for quick test\n",
        "\n",
        "    steps_per_epoch = total_samples // (batch_size * gradient_accumulation)\n",
        "    total_steps = steps_per_epoch * num_epochs\n",
        "\n",
        "    print(f\"\\n‚öôÔ∏è  Quick Training Config:\")\n",
        "    print(f\"   Dataset size: {total_samples:,} samples (10% of full data)\")\n",
        "    print(f\"   Batch size: {batch_size} (effective: {batch_size * gradient_accumulation})\")\n",
        "    print(f\"   Epochs: {num_epochs}\")\n",
        "    print(f\"   Total steps: {total_steps}\")\n",
        "    print(f\"   ‚è±Ô∏è  Estimated time: ~{total_steps * 0.5 / 60:.1f} minutes\")\n",
        "\n",
        "    # Training arguments (optimized for speed)\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "\n",
        "        # ‚ö° FAST TRAINING\n",
        "        num_train_epochs=num_epochs,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        gradient_accumulation_steps=gradient_accumulation,\n",
        "\n",
        "        # ‚ö° LEARNING RATE\n",
        "        learning_rate=5e-5,\n",
        "        warmup_ratio=0.1,\n",
        "        lr_scheduler_type='linear',  # Faster than cosine\n",
        "\n",
        "        # ‚ö° EVALUATION\n",
        "        eval_strategy='epoch',\n",
        "        save_strategy='epoch',\n",
        "        save_total_limit=1,  # Only keep best checkpoint\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='eval_loss',\n",
        "\n",
        "        # ‚ö° LOGGING (minimal)\n",
        "        logging_strategy='epoch',  # Only log per epoch\n",
        "        logging_dir=f'{output_dir}/logs',\n",
        "\n",
        "        # ‚ö° SPEED OPTIMIZATIONS\n",
        "        fp16=False,  # Disable for CPU (faster on CPU)\n",
        "        dataloader_num_workers=0,  # Disable workers on CPU\n",
        "        dataloader_pin_memory=False,\n",
        "\n",
        "        # OTHER\n",
        "        weight_decay=0.01,\n",
        "        report_to='none',\n",
        "        disable_tqdm=False,  # Show progress bar\n",
        "        seed=42,\n",
        "    )\n",
        "\n",
        "    # Early stopping (stop after 2 epochs if no improvement)\n",
        "    early_stopping = EarlyStoppingCallback(\n",
        "        early_stopping_patience=2,\n",
        "        early_stopping_threshold=0.01\n",
        "    )\n",
        "\n",
        "    # Progress callback\n",
        "    progress_callback = QuickProgressCallback()\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        callbacks=[early_stopping, progress_callback]\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    print(f\"\\nüöÄ Starting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # Save\n",
        "    print(f\"\\nüíæ Saving model...\")\n",
        "    trainer.save_model(output_dir)\n",
        "    t5_tokenizer.save_pretrained(f\"{output_dir}/tokenizer\")\n",
        "\n",
        "    # Save metrics\n",
        "    metrics = {\n",
        "        'dataset_size': total_samples,\n",
        "        'subset_percentage': 10,\n",
        "        'epochs_completed': int(trainer.state.epoch),\n",
        "        'best_eval_loss': trainer.state.best_metric,\n",
        "        'training_time_minutes': (time.time() - progress_callback.start_time) / 60\n",
        "    }\n",
        "\n",
        "    with open(f\"{output_dir}/metrics.json\", 'w') as f:\n",
        "        json.dump(metrics, f, indent=2)\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"‚úÖ BASELINE TRAINING COMPLETE\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"   Best validation loss: {trainer.state.best_metric:.4f}\")\n",
        "    print(f\"   Training time: {metrics['training_time_minutes']:.1f} minutes\")\n",
        "\n",
        "    return trainer, model\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# TRAIN BASELINE MODEL\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nüéØ (1) TRAINING BASELINE MODEL ON SMALL DATASET\")\n",
        "\n",
        "baseline_trainer, baseline_model = train_baseline_quick(\n",
        "    train_baseline_dataset,\n",
        "    val_baseline_dataset,\n",
        "    output_dir='models/t5_baseline'\n",
        ")\n",
        "\n",
        "# Clear memory\n",
        "import gc\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\n‚úÖ Baseline model training complete!\")\n",
        "print(\"   Ready to train clarified model next...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0fLAqnc6xm8"
      },
      "outputs": [],
      "source": [
        "# # ============================================================================\n",
        "# # Cell 15: Train Baseline Model (OPTIMIZED WITH PROGRESS MONITORING)\n",
        "# # ============================================================================\n",
        "\n",
        "# import torch\n",
        "# from torch.utils.data import Dataset\n",
        "# from transformers import (\n",
        "#     T5ForConditionalGeneration,\n",
        "#     Trainer,\n",
        "#     TrainingArguments,\n",
        "#     TrainerCallback,\n",
        "#     EarlyStoppingCallback\n",
        "# )\n",
        "# from tqdm.auto import tqdm\n",
        "# import time\n",
        "# import json\n",
        "# import os\n",
        "\n",
        "# # ============================================================================\n",
        "# # CUSTOM CALLBACK: Real-time Progress Display\n",
        "# # ============================================================================\n",
        "\n",
        "# class ProgressCallback(TrainerCallback):\n",
        "#     \"\"\"Display training progress with time estimates.\"\"\"\n",
        "\n",
        "#     def __init__(self):\n",
        "#         self.start_time = None\n",
        "#         self.epoch_start_time = None\n",
        "#         self.best_loss = float('inf')\n",
        "\n",
        "#     def on_train_begin(self, args, state, control, **kwargs):\n",
        "#         self.start_time = time.time()\n",
        "#         print(f\"\\n{'='*70}\")\n",
        "#         print(f\"üöÄ TRAINING STARTED\")\n",
        "#         print(f\"{'='*70}\\n\")\n",
        "\n",
        "#     def on_epoch_begin(self, args, state, control, **kwargs):\n",
        "#         self.epoch_start_time = time.time()\n",
        "#         epoch = int(state.epoch) if state.epoch else 0\n",
        "#         print(f\"\\nüìç Epoch {epoch + 1}/{args.num_train_epochs}\")\n",
        "\n",
        "#     def on_epoch_end(self, args, state, control, **kwargs):\n",
        "#         epoch_time = time.time() - self.epoch_start_time\n",
        "#         epoch = int(state.epoch)\n",
        "\n",
        "#         # Get latest eval loss\n",
        "#         eval_loss = None\n",
        "#         for log in reversed(state.log_history):\n",
        "#             if 'eval_loss' in log:\n",
        "#                 eval_loss = log['eval_loss']\n",
        "#                 break\n",
        "\n",
        "#         print(f\"   ‚è±Ô∏è  Epoch {epoch} completed in {epoch_time/60:.1f} minutes\")\n",
        "#         if eval_loss:\n",
        "#             improvement = \"üìà NEW BEST!\" if eval_loss < self.best_loss else \"\"\n",
        "#             self.best_loss = min(self.best_loss, eval_loss)\n",
        "#             print(f\"   üìä Validation Loss: {eval_loss:.4f} {improvement}\")\n",
        "\n",
        "#     def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "#         \"\"\"Display training loss every 50 steps.\"\"\"\n",
        "#         if logs and 'loss' in logs and state.global_step % 50 == 0:\n",
        "#             elapsed = time.time() - self.start_time\n",
        "#             steps_remaining = state.max_steps - state.global_step\n",
        "#             time_per_step = elapsed / state.global_step if state.global_step > 0 else 0\n",
        "#             eta = steps_remaining * time_per_step / 60\n",
        "\n",
        "#             print(f\"   Step {state.global_step}/{state.max_steps} | \"\n",
        "#                   f\"Loss: {logs['loss']:.4f} | \"\n",
        "#                   f\"ETA: {eta:.1f}min\")\n",
        "\n",
        "#     def on_train_end(self, args, state, control, **kwargs):\n",
        "#         total_time = time.time() - self.start_time\n",
        "#         print(f\"\\n{'='*70}\")\n",
        "#         print(f\"‚úÖ TRAINING COMPLETED\")\n",
        "#         print(f\"{'='*70}\")\n",
        "#         print(f\"   Total time: {total_time/60:.1f} minutes\")\n",
        "#         print(f\"   Best validation loss: {self.best_loss:.4f}\")\n",
        "\n",
        "\n",
        "# # ============================================================================\n",
        "# # OPTIMIZED TRAINING FUNCTION\n",
        "# # ============================================================================\n",
        "\n",
        "# def train_baseline_model_optimized(train_dataset, val_dataset, output_dir='models/t5_baseline'):\n",
        "#     \"\"\"\n",
        "#     Train T5 model with MAXIMUM optimization for speed.\n",
        "\n",
        "#     Optimizations:\n",
        "#     - Larger batch size (8 vs 4)\n",
        "#     - Gradient accumulation (2 steps = effective batch 16)\n",
        "#     - Mixed precision (FP16)\n",
        "#     - Optimized data loading (4 workers + pin memory)\n",
        "#     - Fewer epochs with early stopping\n",
        "#     - Efficient checkpointing\n",
        "#     \"\"\"\n",
        "\n",
        "#     print(f\"\\n{'='*70}\")\n",
        "#     print(f\"TRAINING: BASELINE MODEL (OPTIMIZED)\")\n",
        "#     print(f\"{'='*70}\")\n",
        "\n",
        "#     # ============================================================================\n",
        "#     # 1. INITIALIZE MODEL\n",
        "#     # ============================================================================\n",
        "\n",
        "#     print(\"\\nüì¶ Loading T5-base model...\")\n",
        "#     model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "#     model.resize_token_embeddings(len(t5_tokenizer))\n",
        "\n",
        "#     # ‚ö° OPTIMIZATION: Enable gradient checkpointing to save memory\n",
        "#     model.gradient_checkpointing_enable()\n",
        "\n",
        "#     print(f\"‚úì Model ready with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "\n",
        "#     # ============================================================================\n",
        "#     # 2. CALCULATE TRAINING PARAMETERS\n",
        "#     # ============================================================================\n",
        "\n",
        "#     total_samples = len(train_dataset)\n",
        "#     batch_size = 8  # ‚ö° Larger batch size for speed\n",
        "#     gradient_accumulation = 2  # Effective batch = 16\n",
        "#     effective_batch_size = batch_size * gradient_accumulation\n",
        "\n",
        "#     num_epochs = 5  # ‚ö° Reduced from 10 to 5 (early stopping will handle it)\n",
        "#     steps_per_epoch = total_samples // effective_batch_size\n",
        "#     total_steps = steps_per_epoch * num_epochs\n",
        "\n",
        "#     print(f\"\\n‚öôÔ∏è  Training Configuration:\")\n",
        "#     print(f\"   Dataset size: {total_samples:,} samples\")\n",
        "#     print(f\"   Batch size: {batch_size} (effective: {effective_batch_size})\")\n",
        "#     print(f\"   Epochs: {num_epochs}\")\n",
        "#     print(f\"   Steps per epoch: {steps_per_epoch}\")\n",
        "#     print(f\"   Total steps: {total_steps}\")\n",
        "#     print(f\"   Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
        "\n",
        "#     # Estimate time (0.3s per step for optimized training)\n",
        "#     estimated_minutes = total_steps * 0.3 / 60\n",
        "#     print(f\"   ‚è±Ô∏è  Estimated time: {estimated_minutes:.1f} minutes\")\n",
        "\n",
        "#     # ============================================================================\n",
        "#     # 3. TRAINING ARGUMENTS (OPTIMIZED)\n",
        "#     # ============================================================================\n",
        "\n",
        "#     training_args = TrainingArguments(\n",
        "#         output_dir=output_dir,\n",
        "\n",
        "#         # ‚ö° TRAINING SPEED OPTIMIZATIONS\n",
        "#         num_train_epochs=num_epochs,\n",
        "#         per_device_train_batch_size=batch_size,\n",
        "#         per_device_eval_batch_size=batch_size,\n",
        "#         gradient_accumulation_steps=gradient_accumulation,\n",
        "\n",
        "#         # ‚ö° LEARNING RATE (slightly higher for faster convergence)\n",
        "#         learning_rate=5e-5,\n",
        "#         warmup_ratio=0.1,\n",
        "#         lr_scheduler_type='cosine',\n",
        "\n",
        "#         # ‚ö° MEMORY & SPEED\n",
        "#         fp16=torch.cuda.is_available(),  # Mixed precision\n",
        "#         dataloader_num_workers=4,  # ‚ö° Increased from 2 to 4\n",
        "#         dataloader_pin_memory=True,\n",
        "#         gradient_checkpointing=True,  # ‚ö° Save memory\n",
        "\n",
        "#         # üìä EVALUATION & CHECKPOINTING\n",
        "#         eval_strategy='epoch',\n",
        "#         save_strategy='epoch',\n",
        "#         save_total_limit=2,  # ‚ö° Keep only 2 best checkpoints\n",
        "#         load_best_model_at_end=True,\n",
        "#         metric_for_best_model='eval_loss',\n",
        "#         greater_is_better=False,\n",
        "\n",
        "#         # üìù LOGGING (optimized frequency)\n",
        "#         logging_dir=f'{output_dir}/logs',\n",
        "#         logging_strategy='steps',\n",
        "#         logging_steps=50,\n",
        "#         logging_first_step=True,\n",
        "\n",
        "#         # ‚öôÔ∏è OTHER\n",
        "#         weight_decay=0.01,\n",
        "#         max_grad_norm=1.0,\n",
        "#         report_to='none',\n",
        "#         seed=42,\n",
        "\n",
        "#         # ‚ö° DISABLE UNNECESSARY FEATURES\n",
        "#         push_to_hub=False,\n",
        "#         disable_tqdm=True,  # We use custom progress display\n",
        "#     )\n",
        "\n",
        "#     # ============================================================================\n",
        "#     # 4. INITIALIZE TRAINER WITH CALLBACKS\n",
        "#     # ============================================================================\n",
        "\n",
        "#     print(\"\\nüéØ Initializing trainer with callbacks...\")\n",
        "\n",
        "#     # Early stopping: stop if no improvement for 2 epochs\n",
        "#     early_stopping = EarlyStoppingCallback(\n",
        "#         early_stopping_patience=2,\n",
        "#         early_stopping_threshold=0.001\n",
        "#     )\n",
        "\n",
        "#     # Custom progress display\n",
        "#     progress_callback = ProgressCallback()\n",
        "\n",
        "#     trainer = Trainer(\n",
        "#         model=model,\n",
        "#         args=training_args,\n",
        "#         train_dataset=train_dataset,\n",
        "#         eval_dataset=val_dataset,\n",
        "#         callbacks=[early_stopping, progress_callback]\n",
        "#     )\n",
        "\n",
        "#     # ============================================================================\n",
        "#     # 5. TRAIN MODEL\n",
        "#     # ============================================================================\n",
        "\n",
        "#     print(f\"\\n{'='*70}\")\n",
        "#     print(f\"üöÄ STARTING OPTIMIZED TRAINING\")\n",
        "#     print(f\"{'='*70}\\n\")\n",
        "\n",
        "#     # Start training\n",
        "#     train_result = trainer.train()\n",
        "\n",
        "#     # ============================================================================\n",
        "#     # 6. SAVE MODEL & RESULTS\n",
        "#     # ============================================================================\n",
        "\n",
        "#     print(f\"\\nüíæ Saving model and tokenizer...\")\n",
        "#     trainer.save_model(output_dir)\n",
        "#     t5_tokenizer.save_pretrained(f\"{output_dir}/tokenizer\")\n",
        "\n",
        "#     # Save training metrics\n",
        "#     metrics = {\n",
        "#         'final_train_loss': train_result.training_loss,\n",
        "#         'best_eval_loss': trainer.state.best_metric,\n",
        "#         'total_steps': trainer.state.global_step,\n",
        "#         'epochs_completed': int(trainer.state.epoch),\n",
        "#         'training_time_minutes': train_result.metrics['train_runtime'] / 60\n",
        "#     }\n",
        "\n",
        "#     with open(f\"{output_dir}/training_metrics.json\", 'w') as f:\n",
        "#         json.dump(metrics, f, indent=2)\n",
        "\n",
        "#     print(f\"\\n{'='*70}\")\n",
        "#     print(f\"‚úÖ BASELINE MODEL TRAINING COMPLETE\")\n",
        "#     print(f\"{'='*70}\")\n",
        "#     print(f\"   Model saved to: {output_dir}\")\n",
        "#     print(f\"   Best validation loss: {trainer.state.best_metric:.4f}\")\n",
        "#     print(f\"   Training time: {metrics['training_time_minutes']:.1f} minutes\")\n",
        "#     print(f\"   Epochs completed: {metrics['epochs_completed']}\")\n",
        "\n",
        "#     return trainer, model\n",
        "\n",
        "\n",
        "# # ============================================================================\n",
        "# # EXECUTE TRAINING\n",
        "# # ============================================================================\n",
        "\n",
        "# # Clear GPU memory before starting\n",
        "# if torch.cuda.is_available():\n",
        "#     torch.cuda.empty_cache()\n",
        "#     print(f\"\\nüßπ GPU cache cleared\")\n",
        "#     print(f\"üìä GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB / \"\n",
        "#           f\"{torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "# # Train baseline model\n",
        "# baseline_trainer, baseline_model = train_baseline_model_optimized(\n",
        "#     train_baseline_dataset,\n",
        "#     val_baseline_dataset,\n",
        "#     output_dir='models/t5_baseline'\n",
        "# )\n",
        "\n",
        "# # Clear cache after training\n",
        "# if torch.cuda.is_available():\n",
        "#     torch.cuda.empty_cache()\n",
        "#     print(f\"\\nüßπ GPU cache cleared after training\")\n",
        "\n",
        "# print(\"\\n‚úÖ Ready for next step: Train clarified model!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4x4e22uXuhtA"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Cell 15.5: Visualize Baseline Training Progress\n",
        "# ============================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"BASELINE MODEL TRAINING VISUALIZATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Extract training history from trainer\n",
        "log_history = baseline_trainer.state.log_history\n",
        "\n",
        "# Separate training and validation logs\n",
        "train_logs = [log for log in log_history if 'loss' in log and 'eval_loss' not in log]\n",
        "eval_logs = [log for log in log_history if 'eval_loss' in log]\n",
        "\n",
        "# Extract data\n",
        "train_steps = [log['step'] for log in train_logs]\n",
        "train_loss = [log['loss'] for log in train_logs]\n",
        "eval_steps = [log['step'] for log in eval_logs]\n",
        "eval_loss = [log['eval_loss'] for log in eval_logs]\n",
        "\n",
        "# Create plot\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
        "\n",
        "# Plot training loss\n",
        "ax.plot(train_steps, train_loss, label='Training Loss', linewidth=2, color='#3498db', alpha=0.8)\n",
        "\n",
        "# Plot validation loss\n",
        "ax.plot(eval_steps, eval_loss, label='Validation Loss', linewidth=2.5, color='#e74c3c', marker='o', markersize=6)\n",
        "\n",
        "# Formatting\n",
        "ax.set_xlabel('Training Steps', fontsize=13, fontweight='bold')\n",
        "ax.set_ylabel('Loss', fontsize=13, fontweight='bold')\n",
        "ax.set_title('Baseline Model Training Progress', fontsize=15, fontweight='bold', pad=15)\n",
        "ax.legend(fontsize=11, loc='upper right')\n",
        "ax.grid(alpha=0.3, linestyle='--')\n",
        "ax.set_ylim(bottom=0)\n",
        "\n",
        "# Add annotations for best validation loss\n",
        "best_eval_idx = eval_loss.index(min(eval_loss))\n",
        "best_eval_step = eval_steps[best_eval_idx]\n",
        "best_eval_loss = eval_loss[best_eval_idx]\n",
        "\n",
        "ax.annotate(f'Best: {best_eval_loss:.4f}',\n",
        "            xy=(best_eval_step, best_eval_loss),\n",
        "            xytext=(best_eval_step, best_eval_loss + max(eval_loss) * 0.1),\n",
        "            arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
        "            fontsize=11, fontweight='bold', color='red',\n",
        "            bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('models/t5_baseline/training_curve.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Print statistics\n",
        "print(f\"\\nüìä Training Statistics:\")\n",
        "print(f\"   Initial training loss: {train_loss[0]:.4f}\")\n",
        "print(f\"   Final training loss: {train_loss[-1]:.4f}\")\n",
        "print(f\"   Training loss reduction: {train_loss[0] - train_loss[-1]:.4f}\")\n",
        "print(f\"\\n   Initial validation loss: {eval_loss[0]:.4f}\")\n",
        "print(f\"   Best validation loss: {best_eval_loss:.4f}\")\n",
        "print(f\"   Validation loss reduction: {eval_loss[0] - best_eval_loss:.4f}\")\n",
        "print(f\"   Best model at step: {best_eval_step}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Baseline training curve saved to: models/t5_baseline/training_curve.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RuiCTTA6zlk"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Cell 16: Train Clarified Model (Clarify-and-Link)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n (2) CLARIFY-AND-LINK MODEL\")\n",
        "clarified_trainer, clarified_model = train_entity_linking_model(\n",
        "    train_clarified_dataset,\n",
        "    val_clarified_dataset,\n",
        "    'clarified',\n",
        "    'models/t5_clarified'\n",
        ")\n",
        "\n",
        "# Clear cache\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\n Both models trained successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InARFv7hulm5"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Cell 16.5: Visualize Clarified Model Training Progress\n",
        "# ============================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CLARIFY-AND-LINK MODEL TRAINING VISUALIZATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Extract training history\n",
        "log_history = clarified_trainer.state.log_history\n",
        "\n",
        "# Separate logs\n",
        "train_logs = [log for log in log_history if 'loss' in log and 'eval_loss' not in log]\n",
        "eval_logs = [log for log in log_history if 'eval_loss' in log]\n",
        "\n",
        "# Extract data\n",
        "train_steps = [log['step'] for log in train_logs]\n",
        "train_loss = [log['loss'] for log in train_logs]\n",
        "eval_steps = [log['step'] for log in eval_logs]\n",
        "eval_loss = [log['eval_loss'] for log in eval_logs]\n",
        "\n",
        "# Create plot\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
        "\n",
        "# Plot training loss\n",
        "ax.plot(train_steps, train_loss, label='Training Loss', linewidth=2, color='#3498db', alpha=0.8)\n",
        "\n",
        "# Plot validation loss\n",
        "ax.plot(eval_steps, eval_loss, label='Validation Loss', linewidth=2.5, color='#e74c3c', marker='o', markersize=6)\n",
        "\n",
        "# Formatting\n",
        "ax.set_xlabel('Training Steps', fontsize=13, fontweight='bold')\n",
        "ax.set_ylabel('Loss', fontsize=13, fontweight='bold')\n",
        "ax.set_title('Clarify-and-Link Model Training Progress', fontsize=15, fontweight='bold', pad=15)\n",
        "ax.legend(fontsize=11, loc='upper right')\n",
        "ax.grid(alpha=0.3, linestyle='--')\n",
        "ax.set_ylim(bottom=0)\n",
        "\n",
        "# Add annotations\n",
        "best_eval_idx = eval_loss.index(min(eval_loss))\n",
        "best_eval_step = eval_steps[best_eval_idx]\n",
        "best_eval_loss = eval_loss[best_eval_idx]\n",
        "\n",
        "ax.annotate(f'Best: {best_eval_loss:.4f}',\n",
        "            xy=(best_eval_step, best_eval_loss),\n",
        "            xytext=(best_eval_step, best_eval_loss + max(eval_loss) * 0.1),\n",
        "            arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
        "            fontsize=11, fontweight='bold', color='red',\n",
        "            bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('models/t5_clarified/training_curve.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Print statistics\n",
        "print(f\"\\nüìä Training Statistics:\")\n",
        "print(f\"   Initial training loss: {train_loss[0]:.4f}\")\n",
        "print(f\"   Final training loss: {train_loss[-1]:.4f}\")\n",
        "print(f\"   Training loss reduction: {train_loss[0] - train_loss[-1]:.4f}\")\n",
        "print(f\"\\n   Initial validation loss: {eval_loss[0]:.4f}\")\n",
        "print(f\"   Best validation loss: {best_eval_loss:.4f}\")\n",
        "print(f\"   Validation loss reduction: {eval_loss[0] - best_eval_loss:.4f}\")\n",
        "print(f\"   Best model at step: {best_eval_step}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Clarified training curve saved to: models/t5_clarified/training_curve.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLjY5zQxumHt"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Cell 16.7: Compare Baseline vs Clarified Training\n",
        "# ============================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING COMPARISON: BASELINE VS CLARIFY-AND-LINK\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Extract baseline logs\n",
        "baseline_history = baseline_trainer.state.log_history\n",
        "baseline_eval = [log for log in baseline_history if 'eval_loss' in log]\n",
        "baseline_eval_steps = [log['step'] for log in baseline_eval]\n",
        "baseline_eval_loss = [log['eval_loss'] for log in baseline_eval]\n",
        "\n",
        "# Extract clarified logs\n",
        "clarified_history = clarified_trainer.state.log_history\n",
        "clarified_eval = [log for log in clarified_history if 'eval_loss' in log]\n",
        "clarified_eval_steps = [log['step'] for log in clarified_eval]\n",
        "clarified_eval_loss = [log['eval_loss'] for log in clarified_eval]\n",
        "\n",
        "# Create comparison plot\n",
        "fig, ax = plt.subplots(1, 1, figsize=(14, 7))\n",
        "\n",
        "# Plot both validation losses\n",
        "ax.plot(baseline_eval_steps, baseline_eval_loss,\n",
        "        label='Baseline', linewidth=2.5, color='#3498db', marker='o', markersize=7)\n",
        "ax.plot(clarified_eval_steps, clarified_eval_loss,\n",
        "        label='Clarify-and-Link', linewidth=2.5, color='#e74c3c', marker='s', markersize=7)\n",
        "\n",
        "# Formatting\n",
        "ax.set_xlabel('Training Steps', fontsize=13, fontweight='bold')\n",
        "ax.set_ylabel('Validation Loss', fontsize=13, fontweight='bold')\n",
        "ax.set_title('Training Comparison: Baseline vs Clarify-and-Link', fontsize=15, fontweight='bold', pad=15)\n",
        "ax.legend(fontsize=12, loc='upper right')\n",
        "ax.grid(alpha=0.3, linestyle='--')\n",
        "ax.set_ylim(bottom=0)\n",
        "\n",
        "# Add final loss comparison\n",
        "final_baseline = min(baseline_eval_loss)\n",
        "final_clarified = min(clarified_eval_loss)\n",
        "\n",
        "# Add text box with comparison\n",
        "textstr = f'Best Validation Loss:\\n'\n",
        "textstr += f'Baseline: {final_baseline:.4f}\\n'\n",
        "textstr += f'Clarified: {final_clarified:.4f}\\n'\n",
        "textstr += f'Improvement: {final_baseline - final_clarified:.4f}'\n",
        "\n",
        "props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
        "ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=11,\n",
        "        verticalalignment='top', bbox=props, fontfamily='monospace')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('data/experiments/training_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüìä Final Comparison:\")\n",
        "print(f\"   Baseline best loss: {final_baseline:.4f}\")\n",
        "print(f\"   Clarified best loss: {final_clarified:.4f}\")\n",
        "print(f\"   Loss improvement: {final_baseline - final_clarified:.4f}\")\n",
        "\n",
        "if final_clarified < final_baseline:\n",
        "    improvement = ((final_baseline - final_clarified) / final_baseline) * 100\n",
        "    print(f\"   Relative improvement: {improvement:.1f}% better ‚úÖ\")\n",
        "else:\n",
        "    print(f\"   Note: Clarified model has higher loss\")\n",
        "\n",
        "print(f\"\\n‚úÖ Comparison plot saved to: data/experiments/training_comparison.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwfBWFGT60vQ"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Cell 17: Evaluation (UPDATED FOR NEW FORMAT)\n",
        "# ============================================================================\n",
        "\n",
        "def evaluate_model(model, tokenizer, test_samples, model_name):\n",
        "    \"\"\"Evaluate entity linking model.\"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"EVALUATING: {model_name.upper()}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    model.eval()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    predictions = []\n",
        "    ground_truths = []\n",
        "    correct = 0\n",
        "\n",
        "    print(f\"\\nüîç Running inference on {len(test_samples)} samples...\")\n",
        "\n",
        "    for sample in tqdm(test_samples, desc=\"Evaluating\"):\n",
        "        input_text = sample['input_text']\n",
        "        target_text = sample['target_text']\n",
        "\n",
        "        # Generate prediction\n",
        "        inputs = tokenizer(input_text, return_tensors='pt', max_length=512, truncation=True)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, max_length=20)  # ‚úÖ Shorter for QIDs\n",
        "\n",
        "        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "\n",
        "        predictions.append(prediction)\n",
        "        ground_truths.append(target_text)\n",
        "\n",
        "        # ‚úÖ Check if correct (flexible matching)\n",
        "        # Handles: \"Q170566\" == \"Q170566\" or \"170566\" == \"Q170566\"\n",
        "        pred_clean = prediction.replace('Q', '').replace('.0', '')\n",
        "        truth_clean = target_text.replace('Q', '').replace('.0', '')\n",
        "\n",
        "        if pred_clean == truth_clean:\n",
        "            correct += 1\n",
        "\n",
        "    accuracy = correct / len(predictions) if len(predictions) > 0 else 0\n",
        "\n",
        "    print(f\"\\nüìä Results for {model_name}:\")\n",
        "    print(f\"   Total samples: {len(predictions)}\")\n",
        "    print(f\"   Correct: {correct}\")\n",
        "    print(f\"   Accuracy: {accuracy:.2%}\")\n",
        "\n",
        "    # Show sample predictions\n",
        "    print(f\"\\nüìù Sample predictions:\")\n",
        "    for i in range(min(5, len(predictions))):\n",
        "        pred_clean = predictions[i].replace('Q', '').replace('.0', '')\n",
        "        truth_clean = ground_truths[i].replace('Q', '').replace('.0', '')\n",
        "        match = \"‚úÖ\" if pred_clean == truth_clean else \"‚ùå\"\n",
        "\n",
        "        print(f\"\\n   {match} Example {i+1}:\")\n",
        "        print(f\"      Input: {test_samples[i]['input_text'][:100]}...\")\n",
        "        print(f\"      Predicted: {predictions[i]}\")\n",
        "        print(f\"      Expected: {ground_truths[i]}\")\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'correct': correct,\n",
        "        'total_samples': len(predictions),\n",
        "        'predictions': predictions,\n",
        "        'ground_truths': ground_truths\n",
        "    }\n",
        "\n",
        "\n",
        "# Evaluate both models\n",
        "baseline_results = evaluate_model(baseline_model, t5_tokenizer, test_baseline, 'Baseline')\n",
        "clarified_results = evaluate_model(clarified_model, t5_tokenizer, test_clarified, 'Clarify-and-Link')\n",
        "\n",
        "# Save results\n",
        "results_comparison = {\n",
        "    'baseline': {\n",
        "        'accuracy': baseline_results['accuracy'],\n",
        "        'correct': baseline_results['correct'],\n",
        "        'total_samples': baseline_results['total_samples']\n",
        "    },\n",
        "    'clarified': {\n",
        "        'accuracy': clarified_results['accuracy'],\n",
        "        'correct': clarified_results['correct'],\n",
        "        'total_samples': clarified_results['total_samples']\n",
        "    },\n",
        "    'improvement': {\n",
        "        'accuracy_gain': clarified_results['accuracy'] - baseline_results['accuracy'],\n",
        "        'accuracy_gain_percent': ((clarified_results['accuracy'] - baseline_results['accuracy']) / baseline_results['accuracy'] * 100) if baseline_results['accuracy'] > 0 else 0\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('data/experiments/evaluation_results.json', 'w') as f:\n",
        "    json.dump(results_comparison, f, indent=2)\n",
        "\n",
        "print(\"\\n‚úÖ Evaluation complete! Results saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_L9deTIKtGZB"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Cell 17.5: Debug - Check What Model Is Actually Predicting\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DEBUG: CHECKING MODEL PREDICTIONS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Test a few samples manually\n",
        "print(\"\\nüîç Testing Baseline Model on 5 samples:\\n\")\n",
        "\n",
        "baseline_model.eval()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "baseline_model.to(device)\n",
        "\n",
        "for i in range(min(5, len(test_baseline))):\n",
        "    sample = test_baseline[i]\n",
        "\n",
        "    print(f\"üìù Sample {i+1}:\")\n",
        "    print(f\"   Input text: {sample['input_text'][:120]}...\")\n",
        "    print(f\"   Expected output: {sample['target_text']}\")\n",
        "\n",
        "    # Generate prediction\n",
        "    inputs = t5_tokenizer(sample['input_text'], return_tensors='pt', max_length=512, truncation=True)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Try different generation settings\n",
        "        outputs = t5_tokenizer.batch_decode(\n",
        "            baseline_model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=50,\n",
        "                num_beams=1,\n",
        "                do_sample=False\n",
        "            ),\n",
        "            skip_special_tokens=True\n",
        "        )\n",
        "\n",
        "    prediction = outputs[0].strip()\n",
        "    print(f\"   Model prediction: '{prediction}'\")\n",
        "    print(f\"   Prediction length: {len(prediction)}\")\n",
        "    print(f\"   Match: {'‚úÖ YES' if prediction.replace('Q','').replace('.0','') == sample['target_text'].replace('Q','').replace('.0','') else '‚ùå NO'}\")\n",
        "    print()\n",
        "\n",
        "print(\"\\nüîç Checking data format:\")\n",
        "print(f\"   First input starts with: {test_baseline[0]['input_text'][:50]}\")\n",
        "print(f\"   First target: {test_baseline[0]['target_text']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4NV_0ni62j-"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Cell 18: Results Visualization (FIXED)\n",
        "# ============================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RESULTS VISUALIZATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (14, 6)\n",
        "plt.rcParams['font.size'] = 11\n",
        "\n",
        "# Create comparison plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))  # ‚úÖ Slightly larger\n",
        "\n",
        "# Accuracy comparison\n",
        "models = ['Baseline', 'Clarify-and-Link']\n",
        "accuracies = [baseline_results['accuracy'], clarified_results['accuracy']]\n",
        "colors = ['#3498db', '#e74c3c']\n",
        "\n",
        "axes[0].bar(models, accuracies, color=colors, alpha=0.8, width=0.6)\n",
        "axes[0].set_ylabel('Accuracy', fontsize=13, fontweight='bold')\n",
        "axes[0].set_title('Entity Linking Accuracy', fontsize=15, fontweight='bold', pad=20)  # ‚úÖ Add padding\n",
        "axes[0].set_ylim([0, min(1.0, max(accuracies) * 1.3)])  # ‚úÖ Dynamic y-limit\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, (model, acc) in enumerate(zip(models, accuracies)):\n",
        "    axes[0].text(i, acc + 0.02, f'{acc:.1%}', ha='center', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Improvement visualization\n",
        "improvement = clarified_results['accuracy'] - baseline_results['accuracy']\n",
        "improvement_percent = improvement * 100\n",
        "\n",
        "axes[1].bar(['Accuracy\\nImprovement'], [improvement_percent], color='#2ecc71', alpha=0.8, width=0.5)\n",
        "axes[1].set_ylabel('Percentage Points', fontsize=13, fontweight='bold')\n",
        "axes[1].set_title('Clarify-and-Link Improvement', fontsize=15, fontweight='bold', pad=20)  # ‚úÖ Add padding\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "axes[1].axhline(y=0, color='black', linestyle='-', linewidth=0.8)  # Add zero line\n",
        "\n",
        "# Add value label\n",
        "label_y = improvement_percent + (0.5 if improvement_percent > 0 else -0.5)\n",
        "axes[1].text(0, label_y, f'+{improvement:.1%}' if improvement >= 0 else f'{improvement:.1%}',\n",
        "             ha='center', fontsize=12, fontweight='bold')\n",
        "\n",
        "# ‚úÖ FIX: Use constrained_layout instead of tight_layout\n",
        "plt.subplots_adjust(left=0.08, right=0.95, top=0.88, bottom=0.12, wspace=0.25)\n",
        "\n",
        "plt.savefig('data/experiments/results_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Visualization saved!\")\n",
        "\n",
        "# Print summary\n",
        "print(f\"\\nüìä Final Results:\")\n",
        "print(f\"   Baseline Accuracy: {baseline_results['accuracy']:.2%}\")\n",
        "print(f\"   Clarify-and-Link Accuracy: {clarified_results['accuracy']:.2%}\")\n",
        "print(f\"   Improvement: {'+' if improvement >= 0 else ''}{improvement:.2%}\")\n",
        "print(f\"   Improvement (percentage points): {'+' if improvement_percent >= 0 else ''}{improvement_percent:.2f}pp\")\n",
        "\n",
        "# Additional statistics\n",
        "if baseline_results['accuracy'] > 0:\n",
        "    relative_improvement = (improvement / baseline_results['accuracy']) * 100\n",
        "    print(f\"   Relative improvement: {relative_improvement:.1f}%\")\n",
        "\n",
        "print(f\"\\nüìà Sample Counts:\")\n",
        "print(f\"   Baseline: {baseline_results['correct']}/{baseline_results['total_samples']} correct\")\n",
        "print(f\"   Clarified: {clarified_results['correct']}/{clarified_results['total_samples']} correct\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09-ZbGj864hx"
      },
      "outputs": [],
      "source": [
        "# # ============================================================================\n",
        "# # Cell 19: Download Everything\n",
        "# # ============================================================================\n",
        "\n",
        "# from google.colab import files\n",
        "# import shutil\n",
        "\n",
        "# print(\"üì¶ Creating final package...\")\n",
        "\n",
        "# # Create archive with all results\n",
        "# shutil.make_archive('clarify_and_link_complete', 'zip', 'data/experiments')\n",
        "# shutil.make_archive('trained_models', 'zip', 'models')\n",
        "\n",
        "# print(\"\\nüì• Downloading results...\")\n",
        "# files.download('clarify_and_link_complete.zip')\n",
        "# files.download('trained_models.zip')\n",
        "\n",
        "# print(\"\\n‚úÖ Download complete!\")\n",
        "# print(\"\\nPackage contents:\")\n",
        "# print(\"  clarify_and_link_complete.zip:\")\n",
        "# print(\"    - clarifications_train.json\")\n",
        "# print(\"    - clarifications_val.json\")\n",
        "# print(\"    - clarifications_test.json\")\n",
        "# print(\"    - processed_for_training/ (6 JSONL files)\")\n",
        "# print(\"    - evaluation_results.json\")\n",
        "# print(\"    - results_comparison.png\")\n",
        "# print(\"\\n  trained_models.zip:\")\n",
        "# print(\"    - t5_baseline/ (baseline model)\")\n",
        "# print(\"    - t5_clarified/ (clarify-and-link model)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhnW4a067bQQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
